{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3c68a23-575e-4e5e-b4b3-e6c23c8021d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### PySpark DataFrames: a Comprehensive Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ea7c4c7-821c-4f26-8ef5-6b66f4c42d0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DoubleType, BooleanType\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, column\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6013c4f-f5ad-4ba1-adfb-da9988e058cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e115fb8a-e614-426b-9de9-ee5be61229b7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Row objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce31093e-f7e7-4e3b-8a8d-be210be7294c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:  John\nSalary:  Data scientist\nSalary:  4500\n"
     ]
    }
   ],
   "source": [
    "row = Row(name=\"John\", role=\"Data scientist\", salary=4500)\n",
    "print(\"Name: \", row.name)\n",
    "print(\"Salary: \", row['role'])\n",
    "print(\"Salary: \", row[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17ae84b9-26c1-4a1c-a626-b080275d8726",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "row = Row(name=\"John\", role=\"Data scientist\", salary=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e352ad72-ada3-4b2e-96e3-50c16b4588ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: 4500"
     ]
    }
   ],
   "source": [
    "row = Row(\"John\", \"Data scientist\", 4500)\n",
    "row[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7548dbfe-15d6-4079-8b8b-2bd88d7083ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: John, Salary: 4500\n"
     ]
    }
   ],
   "source": [
    "Employee = Row(\"name\", \"role\", \"salary\")\n",
    "employee1 = Employee(\"John\", \"Data scientist\", 4500)\n",
    "print(\"Name: {}, Salary: {}\".format(employee1.name, employee1.salary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7b4e567-aa53-4da4-ab8d-8e8b5949f250",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4ce0a65-9a9b-4da1-afab-52213130662e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: StructType([StructField('name', StringType(), True), StructField('role', StringType(), True), StructField('salary', IntegerType(), True)])"
     ]
    }
   ],
   "source": [
    "someSchema = StructType([\n",
    "  StructField(\"name\", StringType(), True),\n",
    "  StructField(\"role\", StringType(), True),\n",
    "  StructField(\"salary\", IntegerType(), True)]\n",
    ")\n",
    "someSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcfa3b06-197b-4c56-ad19-bb78f32c1194",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: StructType([StructField('name', StringType(), True), StructField('role', StringType(), True), StructField('salary', IntegerType(), True)])"
     ]
    }
   ],
   "source": [
    "ddl_schema = \"`name` STRING, `role` STRING, `salary` INTEGER\"\n",
    "schema = T._parse_datatype_string(ddl_schema)\n",
    "schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13bf1859-a0b3-4852-a0b0-8058b046db5e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Creating DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4925dd0f-f8d2-44a1-ad24-4cf940ade0e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+----+\n|   _1|            _2|  _3|\n+-----+--------------+----+\n| John|Data scientist|4500|\n|James| Data engineer|3200|\n|Laura|Data scientist|4100|\n|  Ali| Data engineer|3200|\n|Steve|     Developer|3600|\n+-----+--------------+----+\n\n"
     ]
    }
   ],
   "source": [
    "data =[ \n",
    "  (\"John\", \"Data scientist\", 4500),\n",
    "  (\"James\", \"Data engineer\", 3200),\n",
    "  (\"Laura\", \"Data scientist\", 4100),\n",
    "  (\"Ali\", \"Data engineer\", 3200),\n",
    "  (\"Steve\", \"Developer\", 3600)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c38885fc-67ce-46b3-afe1-e7eaf563d1fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+\n| name|          role|salary|\n+-----+--------------+------+\n| John|Data scientist|  4500|\n|James| Data engineer|  3200|\n|Laura|Data scientist|  4100|\n|  Ali| Data engineer|  3200|\n|Steve|     Developer|  3600|\n+-----+--------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "data =[ \n",
    "  (\"John\", \"Data scientist\", 4500),\n",
    "  (\"James\", \"Data engineer\", 3200),\n",
    "  (\"Laura\", \"Data scientist\", 4100),\n",
    "  (\"Ali\", \"Data engineer\", 3200),\n",
    "  (\"Steve\", \"Developer\", 3600)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data).toDF(\"name\",\"role\", \"salary\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42145545-2a40-45fc-98a1-2006c072665a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+\n| name|          role|salary|\n+-----+--------------+------+\n| John|Data scientist|  4500|\n|James| Data engineer|  3200|\n|Laura|Data scientist|  4100|\n|  Ali| Data engineer|  3200|\n|Steve|     Developer|  3600|\n+-----+--------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "cols = [\"name\",\"role\", \"salary\"]\n",
    "df = spark.createDataFrame(data).toDF(*cols)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7fa8b2f-9bcf-4a9e-a09f-f5040ca66e58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+\n| name|          role|salary|\n+-----+--------------+------+\n| John|Data scientist|  4500|\n|James| Data engineer|  3200|\n|Laura|Data scientist|  4100|\n|  Ali| Data engineer|  3200|\n|Steve|     Developer|  3600|\n+-----+--------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "data =[ \n",
    "  (\"John\", \"Data scientist\", 4500),\n",
    "  (\"James\", \"Data engineer\", 3200),\n",
    "  (\"Laura\", \"Data scientist\", 4100),\n",
    "  (\"Ali\", \"Data engineer\", 3200),\n",
    "  (\"Steve\", \"Developer\", 3600)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"name\", StringType(), True),\n",
    "  StructField(\"role\", StringType(), True),\n",
    "  StructField(\"salary\", IntegerType(), True)]\n",
    ")\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d9e4895-8628-448a-afdd-18045d4c0357",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+\n| name|          role|salary|\n+-----+--------------+------+\n| John|Data scientist|  4500|\n|James| Data engineer|  3200|\n|Laura|Data scientist|  4100|\n|  Ali| Data engineer|  3200|\n|Steve|     Developer|  3600|\n+-----+--------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "data =[ \n",
    "  (\"John\", \"Data scientist\", 4500),\n",
    "  (\"James\", \"Data engineer\", 3200),\n",
    "  (\"Laura\", \"Data scientist\", 4100),\n",
    "  (\"Ali\", \"Data engineer\", 3200),\n",
    "  (\"Steve\", \"Developer\", 3600)\n",
    "]\n",
    "\n",
    "ddl_schema = \"`name` STRING, `role` STRING, `salary` INTEGER\"\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "  data,\n",
    "  T._parse_datatype_string(ddl_schema)\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bb3718c-9947-438f-9e88-e9e59dfc7c3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+\n| name|          role|salary|\n+-----+--------------+------+\n| John|Data scientist|  4500|\n|James| Data engineer|  3200|\n|Laura|Data scientist|  4100|\n|  Ali| Data engineer|  3200|\n|Steve|     Developer|  3600|\n+-----+--------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "data =[ \n",
    "  Row(name=\"John\", role=\"Data scientist\", salary=4500),\n",
    "  Row(name=\"James\", role=\"Data engineer\", salary=3200),\n",
    "  Row(name=\"Laura\", role=\"Data scientist\", salary=4100),\n",
    "  Row(name=\"Ali\", role=\"Data engineer\", salary=3200),\n",
    "  Row(name=\"Steve\", role=\"Developer\", salary=3600)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa722daf-eda0-485b-9e89-2aff8df6bace",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+\n| name|          role|salary|\n+-----+--------------+------+\n| John|Data scientist|  4500|\n|James| Data engineer|  3200|\n|Laura|Data scientist|  4100|\n|  Ali| Data engineer|  3200|\n|Steve|     Developer|  3600|\n+-----+--------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "data =[ \n",
    "  Row(\"John\", \"Data scientist\", 4500),\n",
    "  Row(\"James\", \"Data engineer\", 3200),\n",
    "  Row(\"Laura\", \"Data scientist\", 4100),\n",
    "  Row(\"Ali\", \"Data engineer\", 3200),\n",
    "  Row(\"Steve\", \"Developer\", 3600)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data).toDF(\"name\", \"role\", \"salary\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ad46c3a-8769-41b6-a4fa-2a4bbc8b126a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+\n| name|          role|salary|\n+-----+--------------+------+\n| John|Data scientist|  4500|\n|James| Data engineer|  3200|\n|Laura|Data scientist|  4100|\n|  Ali| Data engineer|  3200|\n|Steve|     Developer|  3600|\n+-----+--------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "Employee = Row(\"name\", \"role\", \"salary\")\n",
    "\n",
    "data = [\n",
    "  Employee(\"John\", \"Data scientist\", 4500),\n",
    "  Employee(\"James\", \"Data engineer\", 3200),\n",
    "  Employee(\"Laura\", \"Data scientist\", 4100),\n",
    "  Employee(\"Ali\", \"Data engineer\", 3200),\n",
    "  Employee(\"Steve\", \"Developer\", 3600)\n",
    "  ]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "185f373b-e1ea-4eca-a320-97b736e9b299",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+\n| name|          role|salary|\n+-----+--------------+------+\n| John|Data scientist|  4500|\n|James| Data engineer|  3200|\n|Laura|Data scientist|  4100|\n|  Ali| Data engineer|  3200|\n|Steve|     Developer|  3600|\n+-----+--------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "data =[ \n",
    "  (\"John\", \"Data scientist\", 4500),\n",
    "  (\"James\", \"Data engineer\", 3200),\n",
    "  (\"Laura\", \"Data scientist\", 4100),\n",
    "  (\"Ali\", \"Data engineer\", 3200),\n",
    "  (\"Steve\", \"Developer\", 3600)\n",
    "]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "df = rdd.toDF([\"name\",\"role\", \"salary\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f538412b-1567-46b3-8cfe-bec93e6d6ab7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# createDataFrame() a list of Row/tuple/list/dict* or pandas.DataFrame, unless \n",
    "# schema with DataType is provided. So, this doesn't run\n",
    "\n",
    "# data = [1,2,3,4,5]\n",
    "# df1 = spark.createDataFrame(data).toDF(\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6412768-2c46-4c89-9bac-405e340b2773",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n|value|\n+-----+\n|    1|\n|    2|\n|    3|\n|    4|\n|    5|\n+-----+\n\n"
     ]
    }
   ],
   "source": [
    "data = [1,2,3,4,5]\n",
    "df1 = spark.createDataFrame([(i,) for i in data]).toDF(\"value\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0eb18d8e-e5ac-4b3f-afc2-750a8d802848",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n|value|\n+-----+\n|    1|\n|    2|\n|    3|\n|    4|\n|    5|\n+-----+\n\n"
     ]
    }
   ],
   "source": [
    "data = [1,2,3,4,5]\n",
    "df1 = spark.createDataFrame(data, T.IntegerType())\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ba6da8d-2d28-4380-bb08-6aee59308ed9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This doesn't run\n",
    "# data = [1,2,3,4.0,5]\n",
    "# df1 = spark.createDataFrame(data, T.IntegerType())\n",
    "# df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee1cbca6-0ec7-4f35-8ad7-2c7cdd371527",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n|value|\n+-----+\n|  1.0|\n|  2.0|\n|  3.0|\n|  4.0|\n|  5.0|\n+-----+\n\n"
     ]
    }
   ],
   "source": [
    "data = [1,2,3,4.0,5]\n",
    "df1 = spark.createDataFrame(map(float, data), T.FloatType())\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7672f4ca-6183-4666-b47e-a4a320bc3060",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Empty DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5106750-7b6d-4994-94f4-24f82ddf6848",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n||\n++\n++\n\n"
     ]
    }
   ],
   "source": [
    "# Create empty an dataframe\n",
    "df1 = spark.createDataFrame([], T.StructType([]))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cee87ec2-d36a-4532-bce4-449bfdb801e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n|col1|col2|\n+----+----+\n+----+----+\n\n"
     ]
    }
   ],
   "source": [
    "ddl_schema = \"`col1` INTEGER, `col2` INTEGER\"\n",
    "schema = T._parse_datatype_string(ddl_schema)\n",
    "df1 = spark.createDataFrame([], schema)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71b9525c-c157-402c-82c0-5a85d5abdd01",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffe47c97-ae56-4477-a88e-b9f09a572d15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+------+\n| name|         role|salary|\n+-----+-------------+------+\n| John|         null|  null|\n|James|Data engineer|  3200|\n|Laura|         null|  4100|\n| null|Data engineer|  3200|\n|Steve|    Developer|  3600|\n+-----+-------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "  (\"John\", None, None),\n",
    "  (\"James\", \"Data engineer\", 3200),\n",
    "  (\"Laura\", None, 4100),\n",
    "  (None, \"Data engineer\", 3200),\n",
    "  (\"Steve\", \"Developer\", 3600)\n",
    "  ]\n",
    "\n",
    "df1 = spark.createDataFrame(data).toDF(\"name\", \"role\", \"salary\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89e57f40-2741-4f2d-a528-3a68f3274eeb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+\n| name|          role|salary|\n+-----+--------------+------+\n| John|Data scientist|  null|\n|James| Data engineer|  null|\n|Laura|Data scientist|  null|\n|  Ali| Data engineer|  null|\n|Steve|     Developer|  null|\n+-----+--------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.types as T\n",
    "data =[ \n",
    "  (\"John\", \"Data scientist\", None),\n",
    "  (\"James\", \"Data engineer\", None),\n",
    "  (\"Laura\", \"Data scientist\", None),\n",
    "  (\"Ali\", \"Data engineer\", None),\n",
    "  (\"Steve\", \"Developer\", None)\n",
    "]\n",
    "\n",
    "ddl_schema = \"`name` STRING, `role` STRING, `salary` FLOAT\"\n",
    "df = spark.createDataFrame(\n",
    "  data,\n",
    "  T._parse_datatype_string(ddl_schema)\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45304c71-a04b-4619-a374-4355fab07ee2",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "##### Dispalying DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f9f390c-5192-4edf-aa04-ca022bc85a5e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+\n| name|          role|salary|\n+-----+--------------+------+\n| John|Data scientist|  null|\n|James| Data engineer|  null|\n|Laura|Data scientist|  null|\n+-----+--------------+------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b2a1e89-fe56-49ae-a0bc-471fb30f9adc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------\n name   | John           \n role   | Data scientist \n salary | null           \n-RECORD 1----------------\n name   | James          \n role   | Data engineer  \n salary | null           \n-RECORD 2----------------\n name   | Laura          \n role   | Data scientist \n salary | null           \nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.show(n=3, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "757568bb-87c0-4a12-a629-a382daf4ada0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>role</th><th>salary</th></tr></thead><tbody><tr><td>John</td><td>Data scientist</td><td>null</td></tr><tr><td>James</td><td>Data engineer</td><td>null</td></tr><tr><td>Laura</td><td>Data scientist</td><td>null</td></tr><tr><td>Ali</td><td>Data engineer</td><td>null</td></tr><tr><td>Steve</td><td>Developer</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "John",
         "Data scientist",
         null
        ],
        [
         "James",
         "Data engineer",
         null
        ],
        [
         "Laura",
         "Data scientist",
         null
        ],
        [
         "Ali",
         "Data engineer",
         null
        ],
        [
         "Steve",
         "Developer",
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "role",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"float\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d79550b-1783-4e3a-9eb9-4c4f61f013db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Showing the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d76969bb-22ba-480e-a744-414c984df3bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: string (nullable = true)\n |-- role: string (nullable = true)\n |-- salary: float (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cc6dc94-2995-44c6-953f-12aea4d4f6ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[30]: StructType([StructField('name', StringType(), True), StructField('role', StringType(), True), StructField('salary', FloatType(), True)])"
     ]
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9de2382-480d-4336-9ba7-502a9a835b67",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Shape of DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46138b3b-d11d-4f2c-ae89-6af1114821d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[31]: 5"
     ]
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "687f51a4-9022-4d37-92d7-208f27f5aa6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[32]: ['name', 'role', 'salary']"
     ]
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62f3f2b5-d4e6-433e-a032-62b21f544ede",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[33]: 3"
     ]
    }
   ],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "916e10bc-7698-4f1a-8402-c165e6209fb7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[34]: True"
     ]
    }
   ],
   "source": [
    "# Check if a DataFrame is empty\n",
    "df1 = spark.createDataFrame([], T.StructType([]))\n",
    "df1.count() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12e1fc2a-99c5-4bcd-a22a-4504cf262043",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[35]: True"
     ]
    }
   ],
   "source": [
    "# Check if a DataFrame is empty\n",
    "df1.isEmpty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1396e05-3184-417c-afac-90305e445d8d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Displaying the rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cee043f-e675-4fad-9018-67b1ca5d9382",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[36]: [Row(name='John', role='Data scientist', salary=None),\n Row(name='James', role='Data engineer', salary=None),\n Row(name='Laura', role='Data scientist', salary=None)]"
     ]
    }
   ],
   "source": [
    "df.head(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff554737-bb61-41cb-8de0-b63629ddbe82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[37]: [Row(name='John', role='Data scientist', salary=None),\n Row(name='James', role='Data engineer', salary=None),\n Row(name='Laura', role='Data scientist', salary=None)]"
     ]
    }
   ],
   "source": [
    "df.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07daeabb-a417-4a19-b36b-9b5a3df286f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[38]: Row(name='John', role='Data scientist', salary=None)"
     ]
    }
   ],
   "source": [
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fdf0952-9001-4908-9e14-5ac925716168",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[39]: [Row(name='Ali', role='Data engineer', salary=None),\n Row(name='Steve', role='Developer', salary=None)]"
     ]
    }
   ],
   "source": [
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c37351ea-cf93-4987-b112-9427694ae2db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+\n| name|          role|salary|\n+-----+--------------+------+\n| John|Data scientist|  null|\n|James| Data engineer|  null|\n|Laura|Data scientist|  null|\n+-----+--------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.limit(3).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "329f96a2-f1b3-4269-be51-c0e56717c94e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Calculating the statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b632667c-b12d-4d08-a4c3-606c1e8cb9f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-------------+------+\n|summary| name|         role|salary|\n+-------+-----+-------------+------+\n|  count|    5|            5|     0|\n|   mean| null|         null|  null|\n| stddev| null|         null|  null|\n|    min|  Ali|Data engineer|  null|\n|    max|Steve|    Developer|  null|\n+-------+-----+-------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2cecf66-21aa-4147-a95d-457798edff57",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19964883-93a1-45a3-a9bf-ffd71d4a3773",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[42]: Column<'c1'>"
     ]
    }
   ],
   "source": [
    "col1 = col(\"c1\")\n",
    "col2 = column(\"c2\")\n",
    "col1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd9085da-602b-4633-b2af-01c07115c2d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[43]: ['name', 'role', 'salary']"
     ]
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e2ba992-9f21-4d65-91b7-3f9f52178f75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[44]: Column<'name'>"
     ]
    }
   ],
   "source": [
    "df.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e49ea86c-636f-4b6d-ab7d-a41b534d70af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[45]: Column<'name'>"
     ]
    }
   ],
   "source": [
    "df[\"name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ded7582b-24d2-4ba2-8039-19f164d52205",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Selecting columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "023d8bc8-23cb-4792-abb1-f0ad37f6fed9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n| name|\n+-----+\n| John|\n|James|\n|Laura|\n|  Ali|\n|Steve|\n+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec9f6508-0317-4d42-8cc3-1927e428747c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n| name|\n+-----+\n| John|\n|James|\n|Laura|\n|  Ali|\n|Steve|\n+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e620432-17bb-48cb-a861-7fb58a374402",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n| name|\n+-----+\n| John|\n|James|\n|Laura|\n|  Ali|\n|Steve|\n+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(df[\"name\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e5ffb50-dd5f-469e-a9e5-15d1f915732b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n| name|          role|\n+-----+--------------+\n| John|Data scientist|\n|James| Data engineer|\n|Laura|Data scientist|\n|  Ali| Data engineer|\n|Steve|     Developer|\n+-----+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select([\"name\", \"role\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e972689e-f6cb-4389-8b89-47d831cbbf21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n| name|          role|\n+-----+--------------+\n| John|Data scientist|\n|James| Data engineer|\n|Laura|Data scientist|\n|  Ali| Data engineer|\n|Steve|     Developer|\n+-----+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(\"name\", \"role\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86fffd16-e2d5-470f-90d2-30dfc196a54c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+\n| name|          role|salary|\n+-----+--------------+------+\n| John|Data scientist|  null|\n|James| Data engineer|  null|\n|Laura|Data scientist|  null|\n|  Ali| Data engineer|  null|\n|Steve|     Developer|  null|\n+-----+--------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"*\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1988afc3-81e6-4865-9f1a-b4085fb18dab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n| name|          role|\n+-----+--------------+\n| John|Data scientist|\n|James| Data engineer|\n|Laura|Data scientist|\n|  Ali| Data engineer|\n|Steve|     Developer|\n+-----+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(df.columns[:2]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "080bbac9-dd73-4870-81d9-ccbd322aa220",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n| id|\n+---+\n|1.0|\n|2.0|\n|3.0|\n|4.0|\n|5.0|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "spark.range(1, 6).select(F.col(\"id\").cast(\"double\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cdad8dd-62d6-4ec6-9cae-179dda8dd5aa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Column expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad404983-5e83-4a56-b01e-b018ef870558",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n|((salary * 1.2) + 100)|\n+----------------------+\n|                  null|\n|                  null|\n|                  null|\n|                  null|\n|                  null|\n+----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"salary\") * 1.2 + 100).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4da513a-5035-4f77-a3ef-db514fa61f49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n|((salary * 1.2) + 100)|\n+----------------------+\n|                  null|\n|                  null|\n|                  null|\n|                  null|\n|                  null|\n+----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(df[\"salary\"] * 1.2 + 100).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ee6d91b-82ba-4e78-a802-b80233d7cefc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------+\n| name|((salary * 1.2) + 100)|\n+-----+----------------------+\n| John|                  null|\n|James|                  null|\n|Laura|                  null|\n|  Ali|                  null|\n|Steve|                  null|\n+-----+----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select([\"name\", df[\"salary\"] * 1.2 + 100]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2245bd85-a93d-4285-a9fd-658e07208413",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n|(salary > 4000)|\n+---------------+\n|           null|\n|           null|\n|           null|\n|           null|\n|           null|\n+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(df[\"salary\"] > 4000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d4158b6-ae54-4519-a721-a30f7f1d1e93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+\n|((salary >= 3200) AND (salary <= 3600))|\n+---------------------------------------+\n|                                   null|\n|                                   null|\n|                                   null|\n|                                   null|\n|                                   null|\n+---------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"salary\").between(3200,3600)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26d3cffe-7001-4c2e-9f26-981e6670b572",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n|concat(name, -, role)|\n+---------------------+\n|  John-Data scientist|\n|  James-Data engineer|\n| Laura-Data scientist|\n|    Ali-Data engineer|\n|      Steve-Developer|\n+---------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(F.concat(df[\"name\"], F.lit(\"-\"), df[\"role\"])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8dbe3ee-3aaa-4d45-a80e-808177a390cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Column aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c58b2332-25e4-4031-9483-820ce3e9db62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "col_expr = (col(\"salary\") * 1.2 + 100).alias(\"bonus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c1f958c-a05a-4611-a8e0-b063ec0df688",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n|bonus|\n+-----+\n| null|\n| null|\n| null|\n| null|\n| null|\n+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(col_expr).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53320aa9-4d4b-4888-8971-14bfe9775cac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n|bonus|\n+-----+\n| null|\n| null|\n| null|\n| null|\n| null|\n+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.select((col(\"salary\") * 1.2 + 100).name(\"bonus\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "621600f6-2883-4f11-9492-3a59c1fdeb2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n|(salary > 4000)|\n+---------------+\n|           null|\n|           null|\n|           null|\n|           null|\n|           null|\n+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(F.expr(\"salary > 4000\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50e67d57-7655-4435-8f97-664ce432e1da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### String manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55dc9630-7dbb-4226-85c1-46144c7b4482",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n| name|\n+-----+\n| john|\n|james|\n|laura|\n|  ali|\n|steve|\n+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(F.lower(df['name']).alias('name')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c355853-b381-4fc4-b30e-43b9547b579d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Math functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dcde319-5607-442a-a4c9-1a77cbdb2a57",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|round(ln(number), 3)|\n+--------------------+\n|                 0.0|\n|               0.693|\n|               1.099|\n|               1.386|\n|               1.609|\n+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [1.0,2.0,3.0,4.0,5.0]\n",
    "df1 = spark.createDataFrame(data, FloatType()).toDF(\"number\")\n",
    "df1.select(F.round(F.log(\"number\"), 3)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fd99cd6-2d1e-41ee-bde3-87c925dea5da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n|round(number, -2)|\n+-----------------+\n|              0.0|\n|            200.0|\n|           1300.0|\n+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [0.6892, 206.892, 1268.0]\n",
    "df1 = spark.createDataFrame([(i,) for i in data]).toDF(\"number\")\n",
    "df1.select(F.round(col(\"number\"), -2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63800a27-27d1-4ce0-8850-a4faddf21a54",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Creating new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21651084-22b4-429e-99ab-b33238b82848",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+--------+\n| name|          role|salary|vacation|\n+-----+--------------+------+--------+\n| John|Data scientist|  null|      15|\n|James| Data engineer|  null|      15|\n|Laura|Data scientist|  null|      15|\n|  Ali| Data engineer|  null|      15|\n|Steve|     Developer|  null|      15|\n+-----+--------------+------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df1 = df.withColumn(\"vacation\", F.lit(15))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14fd6dac-084e-4a95-a852-bd859123b2ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+\n| name|          role|salary|\n+-----+--------------+------+\n| John|Data scientist|  null|\n|James| Data engineer|  null|\n|Laura|Data scientist|  null|\n|  Ali| Data engineer|  null|\n|Steve|     Developer|  null|\n+-----+--------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df1 = df.withColumn(\"salary\", col(\"salary\") + 300)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11346efc-ebfa-4ee0-9861-316de6c2e6d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+-----+\n| name|          role|salary|bonus|\n+-----+--------------+------+-----+\n| John|Data scientist|  null| null|\n|James| Data engineer|  null| null|\n|Laura|Data scientist|  null| null|\n|  Ali| Data engineer|  null| null|\n|Steve|     Developer|  null| null|\n+-----+--------------+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df1 = df.select(col(\"*\"), (col(\"salary\") * 1.2 + 100).name(\"bonus\"))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30be96b8-19f8-4ad9-9af9-b44bd2c27399",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+------+------+\n| name|          role|salary|bonus1|bonus2|\n+-----+--------------+------+------+------+\n| John|Data scientist|  null|  null|  null|\n|James| Data engineer|  null|  null|  null|\n|Laura|Data scientist|  null|  null|  null|\n|  Ali| Data engineer|  null|  null|  null|\n|Steve|     Developer|  null|  null|  null|\n+-----+--------------+------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df1 = df.withColumns({\"bonus1\": col(\"salary\") + 100, \"bonus2\": col(\"salary\") + 300})\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f5e6a4b-4750-41c9-ad4c-3494528355a4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1953dc43-bf35-431c-80bf-1fc0b05f8698",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+------+\n|first name|           job|salary|\n+----------+--------------+------+\n|      John|Data scientist|  null|\n|     James| Data engineer|  null|\n|     Laura|Data scientist|  null|\n|       Ali| Data engineer|  null|\n|     Steve|     Developer|  null|\n+----------+--------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df1 =df.withColumnRenamed(\"name\", \"first name\").withColumnRenamed(\"role\", \"job\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "498ecbae-5989-4495-bd77-6db5a6b22583",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Only in Spark version 3.4.0 or later\n",
    "# df1 =df.withColumnsRenamed({\"name\": \"first name\", \"role\": \"job\"})\n",
    "# df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a836a7bb-49e4-4990-82bb-64fccea36d03",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Changing the data type of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3281bf5d-d5e2-47d0-a9f7-daa6df409b6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-----------+\n|time                      |temperature|\n+--------------------------+-----------+\n|2020-01-01 07:30:10.150007|17.0       |\n|2020-01-02 07:30:10.150007|25.5       |\n|2020-01-03 07:30:10.150007|19.5       |\n|2020-01-04 07:30:10.150007|21.2       |\n|2020-01-05 07:30:10.150007|18.0       |\n|2020-01-06 07:30:10.150007|20.5       |\n+--------------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "  (\"2020-01-01 07:30:10.150007\", \"17.0\"), \n",
    "  (\"2020-01-02 07:30:10.150007\", \"25.5\"),  \n",
    "  (\"2020-01-03 07:30:10.150007\", \"19.5\"),  \n",
    "  (\"2020-01-04 07:30:10.150007\", \"21.2\"),  \n",
    "  (\"2020-01-05 07:30:10.150007\", \"18.0\"), \n",
    "  (\"2020-01-06 07:30:10.150007\", \"20.5\")\n",
    "  ]\n",
    "\n",
    "df1 = spark.createDataFrame(data).toDF(\"time\", \"temperature\")\n",
    "df1 = df1.withColumn(\"temperature\", col(\"temperature\").cast(DoubleType()))\n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4523dda4-5b81-495b-8eb8-1501dc793077",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-----------+\n|time                      |temperature|\n+--------------------------+-----------+\n|2020-01-01 07:30:10.150007|17.0       |\n|2020-01-02 07:30:10.150007|25.5       |\n|2020-01-03 07:30:10.150007|19.5       |\n|2020-01-04 07:30:10.150007|21.2       |\n|2020-01-05 07:30:10.150007|18.0       |\n|2020-01-06 07:30:10.150007|20.5       |\n+--------------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "  (\"2020-01-01 07:30:10.150007\", \"17.0\"), \n",
    "  (\"2020-01-02 07:30:10.150007\", \"25.5\"),  \n",
    "  (\"2020-01-03 07:30:10.150007\", \"19.5\"),  \n",
    "  (\"2020-01-04 07:30:10.150007\", \"21.2\"),  \n",
    "  (\"2020-01-05 07:30:10.150007\", \"18.0\"), \n",
    "  (\"2020-01-06 07:30:10.150007\", \"20.5\")\n",
    "  ]\n",
    "\n",
    "df1 = spark.createDataFrame(data).toDF(\"time\", \"temperature\")\n",
    "df1 = df1.withColumn(\"temperature\", col(\"temperature\").cast(\"double\"))\n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f0fa73f-63b8-4a55-b49f-6c7705dc69ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n|col1|col2|\n+----+----+\n| 1.0|   2|\n| 2.0|   4|\n| 3.0|   6|\n| 4.0|   8|\n+----+----+\n\n"
     ]
    }
   ],
   "source": [
    "data =[ \n",
    "  (\"1.0\", 2),\n",
    "  (\"2.0\", 4),\n",
    "  (\"3.0\", 6),\n",
    "  (\"4.0\", 8)\n",
    "]\n",
    "df1 = spark.createDataFrame(data).toDF(\"col1\",\"col2\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1abfd0ce-65c8-4f30-8da9-db5fdb0c0b3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n|CAST((col1 >= 2) AS INT)|\n+------------------------+\n|                       0|\n|                       1|\n|                       1|\n|                       1|\n+------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.select((df1[\"col1\"] >= 2).cast('int')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0950c55c-7271-49c9-8825-c282bc9bb731",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n|(col1 + col2)|\n+-------------+\n|          3.0|\n|          6.0|\n|          9.0|\n|         12.0|\n+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Automatic conversion from string to float\n",
    "df1.select(df1[\"col1\"]+df1[\"col2\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97c73eb8-5631-4da1-9459-33819fdc7f00",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Dropping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fca8061e-5681-4dac-82b5-f83be72b0590",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n| name|\n+-----+\n| John|\n|James|\n|Laura|\n|  Ali|\n|Steve|\n+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.drop(\"role\", \"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12155d5e-c89d-4b76-a78d-eccfda2c7fc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n| name|\n+-----+\n| John|\n|James|\n|Laura|\n|  Ali|\n|Steve|\n+-----+\n\n"
     ]
    }
   ],
   "source": [
    "cols = [\"role\", \"salary\"]\n",
    "df.drop(*cols).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43de1e18-ec52-4954-a8d3-20d4f48da938",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Dropping duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e672851c-3c02-4aa0-be71-75d565ff4441",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+\n| name|          role|salary|\n+-----+--------------+------+\n|James| Data engineer|  3200|\n| John|Data scientist|  4500|\n|Steve|     Developer|  3600|\n+-----+--------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "  (\"John\", \"Data scientist\", 4500),\n",
    "  (\"James\", \"Data engineer\", 3200),\n",
    "  (\"Laura\", \"Data scientist\", 4100),\n",
    "  (\"Ali\", \"Data engineer\", 3200),\n",
    "  (\"Steve\", \"Developer\", 3600),\n",
    "  (\"John\", \"Data scientist\", 4500)\n",
    "  ]\n",
    "\n",
    "df1 = spark.createDataFrame(data).toDF(\"name\", \"role\", \"salary\")\n",
    "df1.dropDuplicates([\"role\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c658878-89ce-4b8f-b0f7-9eb32cb2ce3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+\n| name|          role|salary|\n+-----+--------------+------+\n| John|Data scientist|  4500|\n|James| Data engineer|  3200|\n|Laura|Data scientist|  4100|\n|  Ali| Data engineer|  3200|\n|Steve|     Developer|  3600|\n+-----+--------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c15f230-7cfe-449e-81ef-6c9e7e651155",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n|          role|\n+--------------+\n|Data scientist|\n| Data engineer|\n|     Developer|\n+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.select(\"role\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4151495-e3d2-4dbd-bbcf-edbd3aec3c4b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Splitting a column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6e543b0-e6fe-4f6d-b0e4-09fc6bbb1d78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n|splitted_column        |\n+-----------------------+\n|[John, Data scientist] |\n|[James, Data engineer] |\n|[Laura, Data scientist]|\n+-----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "  (\"John,Data scientist\", 4500),\n",
    "  (\"James,Data engineer\", 3200),\n",
    "  (\"Laura,Data scientist\", 4100)\n",
    "]\n",
    "df1 = spark.createDataFrame(data).toDF(\"name-role\", \"salary\")\n",
    "df1.select(F.split(col(\"name-role\"), \",\") \\\n",
    "   .alias(\"splitted_column\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92923d49-fe43-4c0c-a737-232a1c6be43b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;36m  File \u001B[0;32m\"<command-4080278363775537>\"\u001B[0;36m, line \u001B[0;32m3\u001B[0m\n",
       "\u001B[0;31m    col(\"splitted_column\")[0].alias(\"name\"),\u001B[0m\n",
       "\u001B[0m    ^\u001B[0m\n",
       "\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;36m  File \u001B[0;32m\"<command-4080278363775537>\"\u001B[0;36m, line \u001B[0;32m3\u001B[0m\n\u001B[0;31m    col(\"splitted_column\")[0].alias(\"name\"),\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n",
       "errorSummary": "<span class='ansi-red-fg'>SyntaxError</span>: invalid syntax (<command-4080278363775537>, line 3)",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "split_df = df1.select(F.split(col(\"name-role\"), \",\").alias(\"splitted_column\"), col(\"salary\"))\n",
    "split_df.select(s\n",
    "            col(\"splitted_column\")[0].alias(\"name\"),\n",
    "            col(\"splitted_column\")[1].alias(\"role\"),\n",
    "            col(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a81cc6b3-7c39-4a57-9cd7-3bf6aab2acc1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "\u001B[0;32m<command-4080278363775538>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0;31m \u001B[0msplit_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"splitted_column\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malias\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"col{}\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m: name 'split_df' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-4080278363775538>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0msplit_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"splitted_column\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malias\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"col{}\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;31mNameError\u001B[0m: name 'split_df' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'split_df' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "split_df.select([col(\"splitted_column\")[i].alias(\"col{}\".format(i+1)) for i in range(2)]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63f1f009-4cb0-440c-849f-46c364c7f8cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d4527c9-5727-4bac-a4af-e45acd62c0ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Filtering the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db0b66ca-93f8-415b-8119-7589e4ee6afa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+------+\n| name|         role|salary|\n+-----+-------------+------+\n|James|Data engineer|  3200|\n|  Ali|Data engineer|  3200|\n+-----+-------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "data =[ \n",
    "  (\"John\", \"Data scientist\", 4500),\n",
    "  (\"James\", \"Data engineer\", 3200),\n",
    "  (\"Laura\", \"Data scientist\", 4100),\n",
    "  (\"Ali\", \"Data engineer\", 3200),\n",
    "  (\"Steve\", \"Developer\", 3600)\n",
    "]\n",
    "df = spark.createDataFrame(data).toDF(\"name\",\"role\", \"salary\")\n",
    "\n",
    "df.filter(col(\"salary\") == 3200).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b399c030-2ab4-4fa4-9b98-2e241bd6b036",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+------+\n| name|         role|salary|\n+-----+-------------+------+\n|James|Data engineer|  3200|\n|  Ali|Data engineer|  3200|\n+-----+-------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.where(col(\"salary\") == 3200).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31c16acb-aea7-4b1e-9cf8-541705d93424",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+\n| name|          role|salary|\n+-----+--------------+------+\n| John|Data scientist|  4500|\n|Laura|Data scientist|  4100|\n|Steve|     Developer|  3600|\n+-----+--------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.filter((col(\"salary\") > 3200) & (col(\"name\") != \"Ali\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55f28c5a-4e1b-4da6-9e2f-a72f23267fc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+\n| name|          role|salary|\n+-----+--------------+------+\n| John|Data scientist|  4500|\n|Laura|Data scientist|  4100|\n|Steve|     Developer|  3600|\n+-----+--------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"salary > 3200 and name != 'Ali'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5793a79-a697-4ee0-b3f1-dffc6baf6746",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+------+\n| name|         role|salary|\n+-----+-------------+------+\n|James|Data engineer|  3200|\n|  Ali|Data engineer|  3200|\n|Steve|    Developer|  3600|\n+-----+-------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"salary\").between(3200,3600)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "326ac270-e077-4461-980e-29196dc187f8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### when() and otherwise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "123b90cc-b28e-4376-9362-3dd334642488",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n|role code|\n+---------+\n|        3|\n|     null|\n|        3|\n|     null|\n|     null|\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(F.when(df[\"role\"] == \"Data scientist\", 3) \\\n",
    "           .alias(\"role code\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd8a8c6f-a39d-42da-8ae8-8b967b651cc6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n|role code|\n+---------+\n|        3|\n|        2|\n|        3|\n|        2|\n|        1|\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(F.when(df[\"role\"] == \"Data scientist\", 3) \\\n",
    "           .when(df[\"role\"] == \"Data engineer\", 2)   \\\n",
    "           .otherwise(1).alias(\"role code\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03287e9a-c192-4a9b-9255-b3042688d886",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+\n| name|          role|salary|\n+-----+--------------+------+\n| John|Data scientist|  4550|\n|James| Data engineer|  3200|\n|Laura|Data scientist|  4150|\n|  Ali| Data engineer|  3200|\n|Steve|     Developer|  3600|\n+-----+--------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"salary\",\n",
    "              F.when(df[\"role\"] == \"Data scientist\", df[\"salary\"] + 50) \\\n",
    "               .otherwise(df[\"salary\"])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ec3be2a-5973-4379-bc9a-bf674dc5d58f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Methods for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a74853c8-20b2-4fb9-bb4b-81a6a93a403f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+------+\n| name|         role|salary|\n+-----+-------------+------+\n| John|         null|  null|\n|James|Data engineer|  3200|\n|Laura|         null|  4100|\n| null|Data engineer|  3200|\n|Steve|    Developer|  3600|\n+-----+-------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "  (\"John\", None, None),\n",
    "  (\"James\", \"Data engineer\", 3200),\n",
    "  (\"Laura\", None, 4100),\n",
    "  (None, \"Data engineer\", 3200),\n",
    "  (\"Steve\", \"Developer\", 3600)\n",
    "  ]\n",
    "\n",
    "df1 = spark.createDataFrame(data).toDF(\"name\", \"role\", \"salary\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf1bc15a-2b10-4133-ae57-b1b9e62d05da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+\n| name|role|salary|\n+-----+----+------+\n| John|null|  null|\n|Laura|null|  4100|\n+-----+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.filter(df1[\"role\"].isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f84e632-e758-441d-b13d-aa8d74104b23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+\n| name|role|salary|\n+-----+----+------+\n| John|null|  null|\n|Laura|null|  4100|\n+-----+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.filter(F.expr(\"role IS NULL\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d29ea120-55cd-41f9-a35b-afb0a4de13c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+------+\n| name|         role|salary|\n+-----+-------------+------+\n|James|Data engineer|  3200|\n| null|Data engineer|  3200|\n|Steve|    Developer|  3600|\n+-----+-------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.filter(df1[\"role\"].isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc5a890f-964d-4cfb-afc3-d47e6a4f441b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[100]: <pyspark.sql.dataframe.DataFrameNaFunctions at 0x7fe9d45342b0>"
     ]
    }
   ],
   "source": [
    "df1.na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40f798e9-4e38-4ccb-92e5-8dea173bf126",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+------+\n| name|         role|salary|\n+-----+-------------+------+\n| John|          N/A|  null|\n|James|Data engineer|  3200|\n|Laura|          N/A|  4100|\n|  N/A|Data engineer|  3200|\n|Steve|    Developer|  3600|\n+-----+-------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.na.fill(\"N/A\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24799254-f7dd-4f57-9f48-d90440db5ff9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+------+\n| name|         role|salary|\n+-----+-------------+------+\n| John|          N/A|     0|\n|James|Data engineer|  3200|\n|Laura|          N/A|  4100|\n|  N/A|Data engineer|  3200|\n|Steve|    Developer|  3600|\n+-----+-------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.na.fill({\"name\": \"N/A\", \"role\": \"N/A\", \"salary\": 0}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f05d6ca-59c0-48d1-8507-b4e37d4d8c89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+------+\n| name|         role|salary|\n+-----+-------------+------+\n|James|Data engineer|  3200|\n|Steve|    Developer|  3600|\n+-----+-------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.na.drop().show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4448169c-1d4b-486a-967e-902d52abb3c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+------+\n| name|         role|salary|\n+-----+-------------+------+\n|James|Data engineer|  3200|\n|Laura|         null|  4100|\n| null|Data engineer|  3200|\n|Steve|    Developer|  3600|\n+-----+-------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.na.drop(how=\"all\", subset=[\"role\", \"salary\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd5ae335-4c7e-402a-8b61-9dad3079161d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+------+\n| name|         role|salary|\n+-----+-------------+------+\n|James|Data engineer|  3200|\n|Laura|         null|  4100|\n| null|Data engineer|  3200|\n|Steve|    Developer|  3600|\n+-----+-------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.dropna(how=\"all\", subset=[\"role\", \"salary\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9fbdeba-f3ba-4fdc-8632-ca43533ce75b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### User Defined Functions (UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "841dc00f-cff5-41be-a66c-4cf2b0d0c5a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n|number|\n+------+\n|     1|\n|     2|\n|     3|\n|     4|\n|     5|\n|     6|\n|     7|\n|     8|\n|     9|\n|    10|\n+------+\n\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.range(1, 11).select(F.col(\"id\").cast(\"int\").alias(\"number\"))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9351c8d-6353-4501-8bd1-5b3aa1b3e110",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;36m  File \u001B[0;32m\"<command-3138747455353760>\"\u001B[0;36m, line \u001B[0;32m1\u001B[0m\n",
       "\u001B[0;31m    ≈\u001B[0m\n",
       "\u001B[0m    ^\u001B[0m\n",
       "\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid character '≈' (U+2248)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;36m  File \u001B[0;32m\"<command-3138747455353760>\"\u001B[0;36m, line \u001B[0;32m1\u001B[0m\n\u001B[0;31m    ≈\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid character '≈' (U+2248)\n",
       "errorSummary": "<span class='ansi-red-fg'>SyntaxError</span>: invalid character '≈' (U+2248) (<command-3138747455353760>, line 1)",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "≈\n",
    "# the default return type is string\n",
    "def is_even(x):\n",
    "    return x % 2 == 0\n",
    "is_evenUDF = F.udf(lambda x: is_even(x))\n",
    "df2 = df1.withColumn(\"even\", is_evenUDF(F.col(\"number\")))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18261d46-b44c-4c62-a6b1-7a9978ca723e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n|number| even|\n+------+-----+\n|     1|false|\n|     2| true|\n|     3|false|\n|     4| true|\n|     5|false|\n|     6| true|\n|     7|false|\n|     8| true|\n|     9|false|\n|    10| true|\n+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# You cannot use UDFs in a shared cluster\n",
    "def is_even(x):\n",
    "    return x % 2 == 0\n",
    "is_evenUDF = F.udf(lambda x: is_even(x), BooleanType())\n",
    "df2 = df1.withColumn(\"even\", is_evenUDF(df1[\"number\"]))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36c0b211-3cb4-4c21-878d-865eae277fc0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n|number| even|\n+------+-----+\n|     1|false|\n|     2| true|\n|     3|false|\n|     4| true|\n|     5|false|\n|     6| true|\n|     7|false|\n|     8| true|\n|     9|false|\n|    10| true|\n+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.select(col(\"*\"), is_evenUDF(col(\"number\")).alias(\"even\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "609c8a7d-0e54-4da6-b6ce-26672c422ed8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n|number| even|\n+------+-----+\n|     1|false|\n|     2| true|\n|     3|false|\n|     4| true|\n|     5|false|\n|     6| true|\n|     7|false|\n|     8| true|\n|     9|false|\n|    10| true|\n+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "@F.udf(returnType=BooleanType())\n",
    "def is_even(x):\n",
    "    return x % 2 == 0\n",
    "\n",
    "df2 = df1.withColumn(\"even\", is_even(df1[\"number\"]))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc419b7b-6eeb-4815-a3b7-d24afbfdffbb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+--------------------+\n| name|          role|salary|           name_role|\n+-----+--------------+------+--------------------+\n| John|Data scientist|  4500| John Data scientist|\n|James| Data engineer|  3200| James Data engineer|\n|Laura|Data scientist|  4100|Laura Data scientist|\n|  Ali| Data engineer|  3200|   Ali Data engineer|\n|Steve|     Developer|  3600|     Steve Developer|\n+-----+--------------+------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "@F.udf()\n",
    "def func(name, role):\n",
    "    return name+\" \"+role\n",
    "\n",
    "df2 = df.withColumn(\"name_role\", func(col(\"name\"), col(\"role\")))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76c78ac8-26a2-4193-bdc6-a407df8de0b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### UDFs with non-Column parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5905f0e4-6a75-49f1-8436-c736b0f6dd0a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+\n| name|          role|salary|\n+-----+--------------+------+\n| John|Data scientist|  4700|\n|James| Data engineer|  3400|\n|Laura|Data scientist|  4300|\n|  Ali| Data engineer|  3400|\n|Steve|     Developer|  3800|\n+-----+--------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "def add_const(column, number):\n",
    "    return column+number\n",
    "\n",
    "# It cannot be like this:\n",
    "# def add_const_udf(col, number):\n",
    "#     return F.udf(lambda col, number: add_const(col, number), T.IntegerType())\n",
    "# the udf function should be a lambda func since they are curreid\n",
    "\n",
    "add_const_udf = F.udf(lambda column, number: add_const(column, number), T.IntegerType())\n",
    "df2 = df.withColumn(\"salary\", add_const_udf(F.col(\"salary\"), F.lit(200)))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02a9c4d8-b442-46cf-8f51-2b9206585b1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+\n| name|          role|salary|\n+-----+--------------+------+\n| John|Data scientist|  4700|\n|James| Data engineer|  3400|\n|Laura|Data scientist|  4300|\n|  Ali| Data engineer|  3400|\n|Steve|     Developer|  3800|\n+-----+--------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Everything that is passed to an UDF is interpreted as a column / column name. If you want to pass a literal you have two options:\n",
    "# one is currying\n",
    "def add_const(column, number):\n",
    "    return column+number\n",
    "\n",
    "def add_const_udf(number):\n",
    "    return F.udf(lambda column: add_const(column, number), IntegerType())\n",
    "\n",
    "df2 = df.withColumn(\"salary\", add_const_udf(200)(col(\"salary\")))\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c45b4cd4-9bf5-4d7b-96fd-bcb3ea05bddf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### UDFs and broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba57ba60-e436-478e-954e-f821da90a537",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[114]: [1, 2, 3]"
     ]
    }
   ],
   "source": [
    "v = [1, 2, 3]\n",
    "broadcast_v = spark.sparkContext.broadcast(v)\n",
    "broadcast_v.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa83af41-5bd4-463b-a0d6-9e4dd376d8ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+\n|          role|job_code|\n+--------------+--------+\n|Data scientist|       0|\n| Data engineer|       1|\n|Data scientist|       0|\n| Data engineer|       1|\n|     Developer|       2|\n+--------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "job_codes = {\"Data scientist\": 0, \"Data engineer\": 1, \"Developer\": 2}\n",
    "broadcast_job_codes = spark.sparkContext.broadcast(job_codes)\n",
    "\n",
    "def get_job_code_udf(codes):\n",
    "    return F.udf(lambda role: codes[role])\n",
    "\n",
    "df1 = df.select(F.col(\"role\"), get_job_code_udf(broadcast_job_codes.value)(col(\"role\")).alias(\"job_code\"))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06b83373-53ab-4939-b192-8e59405b7e8c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Null values in UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6a65fee-9ae2-4584-b287-ce87b0e8ad79",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This doesn't run \n",
    "# data = [1, 2, 3, 4, None]\n",
    "# df1 = spark.createDataFrame(data, T.IntegerType()).toDF(\"number\")\n",
    "\n",
    "# is_evenUDF = F.udf(lambda x: x % 2 == 0, T.BooleanType())\n",
    "# df2 = df1.withColumn(\"even\", is_evenUDF(F.col(\"number\")))\n",
    "# df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8384ae8-7771-4b3e-b835-ffd74a800d1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n|number| even|\n+------+-----+\n|     1|false|\n|     2| true|\n|     3|false|\n|     4| true|\n|  null| null|\n+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "data = [1, 2, 3, 4, None]\n",
    "df1 = spark.createDataFrame(data, IntegerType()).toDF(\"number\")\n",
    "def is_even(x):\n",
    "    return x % 2 == 0 if x else None\n",
    "    \n",
    "is_evenUDF = F.udf(lambda x: is_even(x), BooleanType())\n",
    "df2 = df1.withColumn(\"even\", is_evenUDF(F.col(\"number\")))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b31ce83c-09f1-4698-9c3e-1d2bcea9575f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21e05148-5690-4a69-ad93-322d37093be9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-----------+\n|time                      |temperature|\n+--------------------------+-----------+\n|2020-01-01 07:30:10.150007|17.0       |\n|2020-01-02 07:30:10.150007|25.5       |\n|2020-01-03 07:30:10.150007|19.5       |\n|2020-01-04 07:30:10.150007|21.2       |\n|2020-01-05 07:30:10.150007|18.0       |\n|2020-01-06 07:30:10.150007|20.5       |\n+--------------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "  (\"2020-01-01 07:30:10.150007\", \"17.0\"), \n",
    "  (\"2020-01-02 07:30:10.150007\", \"25.5\"),  \n",
    "  (\"2020-01-03 07:30:10.150007\", \"19.5\"),  \n",
    "  (\"2020-01-04 07:30:10.150007\", \"21.2\"),  \n",
    "  (\"2020-01-05 07:30:10.150007\", \"18.0\"), \n",
    "  (\"2020-01-06 07:30:10.150007\", \"20.5\")\n",
    "  ]\n",
    "df1 = spark.createDataFrame(data).toDF(\"time\", \"temperature\")\n",
    "\n",
    "df1 = df1.withColumn(\"time\", col(\"time\").cast(\"timestamp\"))\n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12dafe52-1e7c-438b-b398-311bebc29d33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-----------+-------------------+-------------------+\n|time                      |temperature|hour               |minute             |\n+--------------------------+-----------+-------------------+-------------------+\n|2020-01-01 07:30:10.150007|17.0       |2020-01-01 07:00:00|2020-01-01 07:30:00|\n|2020-01-02 07:30:10.150007|25.5       |2020-01-02 07:00:00|2020-01-02 07:30:00|\n|2020-01-03 07:30:10.150007|19.5       |2020-01-03 07:00:00|2020-01-03 07:30:00|\n|2020-01-04 07:30:10.150007|21.2       |2020-01-04 07:00:00|2020-01-04 07:30:00|\n|2020-01-05 07:30:10.150007|18.0       |2020-01-05 07:00:00|2020-01-05 07:30:00|\n|2020-01-06 07:30:10.150007|20.5       |2020-01-06 07:00:00|2020-01-06 07:30:00|\n+--------------------------+-----------+-------------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.withColumn(\"hour\", F.date_trunc('hour', col('time'))) \\\n",
    "         .withColumn(\"minute\", F.date_trunc('minute', col('time')))\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f451fd5-389d-4432-9307-794685b17188",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-----------+-------------------+\n|time                      |temperature|time1              |\n+--------------------------+-----------+-------------------+\n|2020-01-01 07:30:10.150007|17.0       |2020-01-01 08:00:00|\n|2020-01-02 07:30:10.150007|25.5       |2020-01-02 08:00:00|\n|2020-01-03 07:30:10.150007|19.5       |2020-01-03 08:00:00|\n|2020-01-04 07:30:10.150007|21.2       |2020-01-04 08:00:00|\n|2020-01-05 07:30:10.150007|18.0       |2020-01-05 08:00:00|\n|2020-01-06 07:30:10.150007|20.5       |2020-01-06 08:00:00|\n+--------------------------+-----------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Round the time column to nearest hour\n",
    "df2 = df1.withColumn('time1', F.date_trunc('hour', col('time') + F.expr('INTERVAL 30 MINUTES')))\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64621c88-c55b-467f-becc-91f8e8ecf10e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Extracting timestamp parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52d6fc7d-b49f-48b3-922b-f381a9a4cdf5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-----------+----+-----+---+----+-----------+----+------+------+\n|time                      |temperature|year|month|day|week|day of week|hour|minute|second|\n+--------------------------+-----------+----+-----+---+----+-----------+----+------+------+\n|2020-01-01 07:30:10.150007|17.0       |2020|1    |1  |1   |4          |7   |30    |10    |\n|2020-01-02 07:30:10.150007|25.5       |2020|1    |2  |1   |5          |7   |30    |10    |\n|2020-01-03 07:30:10.150007|19.5       |2020|1    |3  |1   |6          |7   |30    |10    |\n|2020-01-04 07:30:10.150007|21.2       |2020|1    |4  |1   |7          |7   |30    |10    |\n|2020-01-05 07:30:10.150007|18.0       |2020|1    |5  |1   |1          |7   |30    |10    |\n|2020-01-06 07:30:10.150007|20.5       |2020|1    |6  |2   |2          |7   |30    |10    |\n+--------------------------+-----------+----+-----+---+----+-----------+----+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.withColumn(\"year\", F.year(df1[\"time\"])) \\\n",
    "         .withColumn(\"month\", F.month(df1[\"time\"])) \\\n",
    "         .withColumn(\"day\", F.dayofmonth(df1[\"time\"])) \\\n",
    "         .withColumn(\"week\", F.weekofyear(df1[\"time\"])) \\\n",
    "         .withColumn(\"day of week\", F.dayofweek(df1[\"time\"])) \\\n",
    "         .withColumn(\"hour\", F.hour(df1[\"time\"])) \\\n",
    "         .withColumn(\"minute\", F.minute(df1[\"time\"])) \\\n",
    "         .withColumn(\"second\", F.second(df1[\"time\"])) \n",
    "\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c722fc3-8c8c-49f8-bffb-4bf2eccf8b68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+\n|unix_timestamp(time, yyyy-MM-dd HH:mm:ss)|\n+-----------------------------------------+\n|                               1577863810|\n|                               1577950210|\n|                               1578036610|\n|                               1578123010|\n|                               1578209410|\n|                               1578295810|\n+-----------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Total time in seconds\n",
    "df1.select(F.unix_timestamp(col('time'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ae066ec-fc6c-4243-8d06-7606f673e3ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n|      time|\n+----------+\n|1577863810|\n|1577950210|\n|1578036610|\n|1578123010|\n|1578209410|\n|1578295810|\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Total time in seconds\n",
    "df1.select(col('time').cast('long')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69e897de-5bf4-4d7b-880c-e784c9302783",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n|microseconds|\n+------------+\n|    0.150007|\n|    0.150007|\n|    0.150007|\n|    0.150007|\n|    0.150007|\n|    0.150007|\n+------------+\n\n"
     ]
    }
   ],
   "source": [
    "microsec_str = F.when(F.split(df1['time'], '[.]').getItem(1).isNull() , 0) \\\n",
    "                .otherwise(F.split(df1['time'], '[.]').getItem(1))\n",
    "df1.select((microsec_str/ 10**F.length(microsec_str)).alias(\"microseconds\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4f5e360-f14d-4713-8d30-7d8210c45593",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Changing the time zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afa213b8-3a01-45fd-99cf-06cb7c28f5cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-----------+\n|time                      |temperature|\n+--------------------------+-----------+\n|2020-01-01 02:30:10.150007|17.0       |\n|2020-01-02 02:30:10.150007|25.5       |\n|2020-01-03 02:30:10.150007|19.5       |\n|2020-01-04 02:30:10.150007|21.2       |\n|2020-01-05 02:30:10.150007|18.0       |\n|2020-01-06 02:30:10.150007|20.5       |\n+--------------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Assuming time is in UTC\n",
    "df2 = df1.withColumn(\"time\", F.from_utc_timestamp(df1[\"time\"], \"America/New_York\"))\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2c54393-2a0c-47b5-a202-3a7ebb0fde11",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-----------+\n|time                      |temperature|\n+--------------------------+-----------+\n|2020-01-01 07:30:10.150007|17.0       |\n|2020-01-02 07:30:10.150007|25.5       |\n|2020-01-03 07:30:10.150007|19.5       |\n|2020-01-04 07:30:10.150007|21.2       |\n|2020-01-05 07:30:10.150007|18.0       |\n|2020-01-06 07:30:10.150007|20.5       |\n+--------------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Going back to UTC\n",
    "df2 = df2.withColumn(\"time\", F.to_utc_timestamp(df2[\"time\"], \"America/New_York\"))\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fdf9edf-3889-430d-9822-6e8a5c394f9a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Creating a date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec6d1f0f-d8c7-463a-a7e0-6cf167fa5894",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n|      date|\n+----------+\n|2018-01-01|\n|2018-02-01|\n|2018-03-01|\n|2018-04-01|\n|2018-05-01|\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "start_dt='2018-01-01'\n",
    "end_dt='2018-05-01'\n",
    "query_str = \"SELECT explode(sequence(to_date('{}'), to_date('{}'), interval 1 month)) as date\".format(start_dt, end_dt)\n",
    "df1 = spark.sql(query_str)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10927bac-ea78-4971-b8cf-ae345a689391",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f21a2451-e55a-4255-9b38-0b4fcf4e7c8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[128]: 8"
     ]
    }
   ],
   "source": [
    "df1 = spark.range(1, 11).select(col(\"id\").cast(\"int\").alias(\"number\"))\n",
    "df1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d97eb12-d3e4-4dc3-ab03-e965e9d4dc63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[129]: 3"
     ]
    }
   ],
   "source": [
    "df2 = df1.repartition(3)\n",
    "df2.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f93520d5-67a6-4bed-b87c-b774eff9dee1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Adding an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e560f88a-8e17-40b3-9764-30b5578f2e70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n|number|         id|\n+------+-----------+\n|     5|          0|\n|     6|          1|\n|     7|          2|\n|     9|          3|\n|     1| 8589934592|\n|     2| 8589934593|\n|     4| 8589934594|\n|     3|17179869184|\n|     8|17179869185|\n|    10|17179869186|\n+------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.range(1, 11).select(col(\"id\").cast(\"int\").alias(\"number\"))\n",
    "df2 = df1.repartition(3)\n",
    "df2 = df2.withColumn(\"id\", F.monotonically_increasing_id())\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62a23332-0010-410a-9b54-beb2c144557b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n|number|         id|\n+------+-----------+\n|     1|          0|\n|     2|          1|\n|     3|          2|\n|     4| 8589934592|\n|     5| 8589934593|\n|     6| 8589934594|\n|     7|17179869184|\n|     8|17179869185|\n|     9|17179869186|\n|    10|25769803776|\n|    11|25769803777|\n|    12|25769803778|\n+------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.range(1, 13).select(col(\"id\").cast(\"int\").alias(\"number\")).coalesce(4)\n",
    "df2 = df1.withColumn(\"id\", F.monotonically_increasing_id())\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aee103e4-8c08-410e-91cb-07ea431c95b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n|number|        id|\n+------+----------+\n|     1|         0|\n|     3|         1|\n|     4|         2|\n|     5|         3|\n|     7|         4|\n|     8|         5|\n|    10|         6|\n|    11|         7|\n|     2|8589934592|\n|     6|8589934593|\n|     9|8589934594|\n|    12|8589934595|\n+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.repartition(2)\n",
    "df2 = df2.withColumn(\"id\", F.monotonically_increasing_id())\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "025ec939-a2ee-4831-883b-534fed1cf32b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n|number|        id|\n+------+----------+\n|     1|         0|\n|     2|         1|\n|     3|         2|\n|     4|         3|\n|     5|         4|\n|     6|         5|\n|     7|8589934592|\n|     8|8589934593|\n|     9|8589934594|\n|    10|8589934595|\n|    11|8589934596|\n|    12|8589934597|\n+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.coalesce(2)\n",
    "df2 = df2.withColumn(\"id\", F.monotonically_increasing_id())\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41a378e8-11a4-4836-91c7-72b8b5d533cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+\n| name|          role|salary|\n+-----+--------------+------+\n| John|Data scientist|  4500|\n|James| Data engineer|  3200|\n|Laura|Data scientist|  4100|\n|  Ali| Data engineer|  3200|\n|Steve|     Developer|  3600|\n+-----+--------------+------+\n\nOut[134]: [(Row(name='John', role='Data scientist', salary=4500), 0),\n (Row(name='James', role='Data engineer', salary=3200), 1),\n (Row(name='Laura', role='Data scientist', salary=4100), 2),\n (Row(name='Ali', role='Data engineer', salary=3200), 3),\n (Row(name='Steve', role='Developer', salary=3600), 4)]"
     ]
    }
   ],
   "source": [
    "data =[ \n",
    "  (\"John\", \"Data scientist\", 4500),\n",
    "  (\"James\", \"Data engineer\", 3200),\n",
    "  (\"Laura\", \"Data scientist\", 4100),\n",
    "  (\"Ali\", \"Data engineer\", 3200),\n",
    "  (\"Steve\", \"Developer\", 3600)\n",
    "]\n",
    "df = spark.createDataFrame(data).toDF(\"name\",\"role\", \"salary\")\n",
    "df.show()\n",
    "\n",
    "zipped_rdd = df.rdd.zipWithIndex()\n",
    "zipped_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3aa87185-0260-4c7c-9bbd-cde9795a2a73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------+------+\n| id| name|          role|salary|\n+---+-----+--------------+------+\n|  0| John|Data scientist|  4500|\n|  1|James| Data engineer|  3200|\n|  2|Laura|Data scientist|  4100|\n|  3|  Ali| Data engineer|  3200|\n|  4|Steve|     Developer|  3600|\n+---+-----+--------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "rdd = df.rdd.zipWithIndex().map(lambda row: (row[1],) + tuple(row[0]))\n",
    "df2 = spark.createDataFrame(rdd, schema= ['id']+ list(df.columns))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edef182c-0a03-49a7-97a2-6b674b7cfc7e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06baec90-5ee2-45d9-9537-2fc4c27a700b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+\n| name|          role|salary|\n+-----+--------------+------+\n|  Ali| Data engineer|  3200|\n|James| Data engineer|  3200|\n| John|Data scientist|  4500|\n|Laura|Data scientist|  4100|\n|Steve|     Developer|  3600|\n+-----+--------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.sort(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f17059c4-8df3-40b3-92b3-f6df02040b42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+\n| name|          role|salary|\n+-----+--------------+------+\n|  Ali| Data engineer|  3200|\n|James| Data engineer|  3200|\n| John|Data scientist|  4500|\n|Laura|Data scientist|  4100|\n|Steve|     Developer|  3600|\n+-----+--------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.sort(df[\"name\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d89b8ad7-1fb1-4fdb-a8a7-7b6b7706fda8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+\n| name|          role|salary|\n+-----+--------------+------+\n|  Ali| Data engineer|  3200|\n|James| Data engineer|  3200|\n|Steve|     Developer|  3600|\n|Laura|Data scientist|  4100|\n| John|Data scientist|  4500|\n+-----+--------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.sort(\"salary\", \"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47caa788-7f8e-4887-8115-399543ff09e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+\n| name|          role|salary|\n+-----+--------------+------+\n|Steve|     Developer|  3600|\n|Laura|Data scientist|  4100|\n| John|Data scientist|  4500|\n|James| Data engineer|  3200|\n|  Ali| Data engineer|  3200|\n+-----+--------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.sort(df[\"name\"].desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d121efd-e058-4063-bf26-dc8db0494d99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+\n| name|          role|salary|\n+-----+--------------+------+\n|Steve|     Developer|  3600|\n|Laura|Data scientist|  4100|\n| John|Data scientist|  4500|\n|James| Data engineer|  3200|\n|  Ali| Data engineer|  3200|\n+-----+--------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.sort(F.desc(\"name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ae602f8-bdc2-4b1b-8cb5-1750a595f298",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0053feaf-78ed-4a62-bbb5-acdcba4fa077",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n|          role|count|\n+--------------+-----+\n|Data scientist|    2|\n| Data engineer|    2|\n|     Developer|    1|\n+--------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"role\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56e95a0a-a092-4e1a-801e-336b911732d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+-----+\n|          role|salary|count|\n+--------------+------+-----+\n|Data scientist|  4500|    1|\n| Data engineer|  3200|    2|\n|Data scientist|  4100|    1|\n|     Developer|  3600|    1|\n+--------------+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"role\", \"salary\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3419a11e-f298-4163-9b2a-b7494b0fd7fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+-----------+\n|          role|count(salary)|avg(salary)|\n+--------------+-------------+-----------+\n|Data scientist|            2|     4300.0|\n| Data engineer|            2|     3200.0|\n|     Developer|            1|     3600.0|\n+--------------+-------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"role\").agg(F.count(\"salary\"), F.avg(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a012bb69-2ed9-4f9e-a86a-875c34811a78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|sum(salary)|\n+-----------+\n|      18600|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.agg(F.sum(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "994aa0bb-e1ae-4563-a5e9-21cd3a3f27a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|sum(salary)|\n+-----------+\n|      18600|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(F.sum(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a60e253-3a7b-46a5-b035-bb73467d3f2a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Pivoting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "000701ac-5b35-4662-9844-51d41a860056",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+\n|product_id|quantity|     city|\n+----------+--------+---------+\n|        P1|     100|Vancouver|\n|        P2|     150|Vancouver|\n|        P3|     130|Vancouver|\n|        P4|     190|Vancouver|\n|        P1|      50|  Toronto|\n|        P2|      60|  Toronto|\n|        P3|      70|  Toronto|\n|        P4|      60|  Toronto|\n|        P1|      30|  Calgary|\n|        P2|     140|  Calgary|\n+----------+--------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [(\"P1\", 100, \"Vancouver\"), (\"P2\", 150, \"Vancouver\"), (\"P3\", 130, \"Vancouver\"), \n",
    "        (\"P4\", 190, \"Vancouver\"), (\"P1\", 50, \"Toronto\"), \n",
    "        (\"P2\", 60, \"Toronto\"), (\"P3\", 70, \"Toronto\"), (\"P4\", 60, \"Toronto\"), \n",
    "        (\"P1\", 30, \"Calgary\"), (\"P2\", 140 ,\"Calgary\")]\n",
    " \n",
    "product_df = spark.createDataFrame(data).toDF(\"product_id\", \"quantity\", \"city\")\n",
    "product_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ab05ad8-d02f-49bd-a169-7dcc46f9ab20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+---------+\n|product_id|Calgary|Toronto|Vancouver|\n+----------+-------+-------+---------+\n|        P4|   null|     60|      190|\n|        P3|   null|     70|      130|\n|        P1|     30|     50|      100|\n|        P2|    140|     60|      150|\n+----------+-------+-------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "pivot_df = product_df.groupBy(\"product_id\").pivot(\"city\").sum(\"quantity\")\n",
    "pivot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14213789-f7de-4481-a3d0-89d53088a582",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+---------+\n|product_id|Calgary|Toronto|Vancouver|\n+----------+-------+-------+---------+\n|        P4|   null|     60|      190|\n|        P3|   null|     70|      130|\n|        P1|     30|     50|      100|\n|        P2|    140|     60|      150|\n+----------+-------+-------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "cities = [\"Calgary\", \"Toronto\", \"Vancouver\"]\n",
    "pivot_df =product_df.groupBy(\"product_id\").pivot(\"city\", cities).sum(\"quantity\")\n",
    "pivot_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b141552c-b1ba-4832-9a04-e1ffaacc6461",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Unpivoting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "527932ee-359b-4e35-b1ed-aff957c860a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+\n|product_id|     city|quantity|\n+----------+---------+--------+\n|        P4|Vancouver|     190|\n|        P4|  Toronto|      60|\n|        P3|Vancouver|     130|\n|        P3|  Toronto|      70|\n|        P1|Vancouver|     100|\n|        P1|  Toronto|      50|\n|        P1|  Calgary|      30|\n|        P2|Vancouver|     150|\n|        P2|  Toronto|      60|\n|        P2|  Calgary|     140|\n+----------+---------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "expr = \"stack(3, 'Vancouver', Vancouver, 'Toronto', Toronto, 'Calgary', Calgary) as (city, quantity)\"\n",
    "unpivot_df = pivot_df.select(F.col(\"product_id\"), F.expr(expr)) \\\n",
    "                     .filter(F.col(\"quantity\").isNotNull())\n",
    "unpivot_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5bd23c7-d807-4532-ab87-b94738a661c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Window functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d883882b-f00c-4f1a-9326-75dc8005eedb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+\n|               time|temperature|\n+-------------------+-----------+\n|2020-01-01 07:30:00|       17.0|\n|2020-01-02 07:30:00|       25.5|\n|2020-01-03 07:30:00|       19.5|\n|2020-01-04 07:30:00|       21.2|\n|2020-01-05 07:30:00|       18.0|\n|2020-01-06 07:30:00|       20.5|\n+-------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "  (\"2020-01-01 07:30:00\", \"17.0\"), \n",
    "  (\"2020-01-02 07:30:00\", \"25.5\"),  \n",
    "  (\"2020-01-03 07:30:00\", \"19.5\"),  \n",
    "  (\"2020-01-04 07:30:00\", \"21.2\"),  \n",
    "  (\"2020-01-05 07:30:00\", \"18.0\"), \n",
    "  (\"2020-01-06 07:30:00\", \"20.5\")\n",
    "  ]\n",
    "\n",
    "df1 = spark.createDataFrame(data).toDF(\"time\", \"temperature\")\n",
    "df1 = df1.withColumn(\"temperature\", col(\"temperature\").cast(DoubleType()))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47d91287-d222-48fd-b21c-b67f2fafbc37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+---------------+\n|               time|temperature|rolling_average|\n+-------------------+-----------+---------------+\n|2020-01-01 07:30:00|       17.0|           17.0|\n|2020-01-02 07:30:00|       25.5|          21.25|\n|2020-01-03 07:30:00|       19.5|          20.67|\n|2020-01-04 07:30:00|       21.2|          22.07|\n|2020-01-05 07:30:00|       18.0|          19.57|\n|2020-01-06 07:30:00|       20.5|           19.9|\n+-------------------+-----------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "w = Window.orderBy(\"time\").rowsBetween(-2, Window.currentRow)  \n",
    "df1.withColumn(\"rolling_average\", F.round(F.avg(\"temperature\").over(w), 2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "128651e5-ef4b-4ec4-87f7-ca9ade2501ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+---------------+\n|               time|temperature|rolling_average|\n+-------------------+-----------+---------------+\n|2020-01-06 07:30:00|       20.5|           20.5|\n|2020-01-05 07:30:00|       18.0|          19.25|\n|2020-01-04 07:30:00|       21.2|           19.9|\n|2020-01-03 07:30:00|       19.5|          19.57|\n|2020-01-02 07:30:00|       25.5|          22.07|\n|2020-01-01 07:30:00|       17.0|          20.67|\n+-------------------+-----------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "w = Window.orderBy(df1[\"time\"].desc()).rowsBetween(-2, Window.currentRow)  \n",
    "df1.withColumn(\"rolling_average\", F.round(F.avg(\"temperature\").over(w), 2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da349fe4-19cb-400d-ad03-53c4699fd21f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+-------+\n|               time|temperature|cum_sum|\n+-------------------+-----------+-------+\n|2020-01-01 07:30:00|       17.0|   17.0|\n|2020-01-02 07:30:00|       25.5|   42.5|\n|2020-01-03 07:30:00|       19.5|   62.0|\n|2020-01-04 07:30:00|       21.2|   83.2|\n|2020-01-05 07:30:00|       18.0|  101.2|\n|2020-01-06 07:30:00|       20.5|  121.7|\n+-------------------+-----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "w = Window.orderBy(\"time\")\n",
    "df1.withColumn(\"cum_sum\", F.sum(\"temperature\").over(w)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f87e4884-3b91-4ce9-b42a-2de212df6f8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+----------+\n| name|          role|salary|row_number|\n+-----+--------------+------+----------+\n|James| Data engineer|  3200|         1|\n|  Ali| Data engineer|  3200|         2|\n|Laura|Data scientist|  4100|         1|\n| John|Data scientist|  4500|         2|\n|Steve|     Developer|  3600|         1|\n+-----+--------------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "data =[ \n",
    "  (\"John\", \"Data scientist\", 4500),\n",
    "  (\"James\", \"Data engineer\", 3200),\n",
    "  (\"Laura\", \"Data scientist\", 4100),\n",
    "  (\"Ali\", \"Data engineer\", 3200),\n",
    "  (\"Steve\", \"Developer\", 3600)\n",
    "]\n",
    "df = spark.createDataFrame(data).toDF(\"name\",\"role\", \"salary\")\n",
    "\n",
    "window_spec  = Window.partitionBy(\"role\").orderBy(\"salary\")\n",
    "df.withColumn(\"row_number\", F.row_number().over(window_spec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e63f8574-5e48-4f51-a554-ad77835ad70c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----------+\n| id|num|rolling_sum|\n+---+---+-----------+\n|  1|  1|          3|\n|  2|  2|          5|\n|  2|  3|          7|\n|  2|  4|          9|\n|  4|  5|         11|\n|  6|  6|         13|\n|  7|  7|          7|\n+---+---+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 1),(2, 2), (2, 3), (2, 4), (4, 5), (6, 6), (7, 7)]\n",
    "df1 = spark.createDataFrame(data).toDF(\"id\", \"num\")\n",
    "w = Window.orderBy(\"id\").rowsBetween(Window.currentRow, 1)  \n",
    "df1.withColumn(\"rolling_sum\", F.sum(\"num\").over(w)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81b1341a-0584-4fd7-b33c-fa73546d8358",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----------+\n| id|num|rolling_sum|\n+---+---+-----------+\n|  1|  1|         10|\n|  2|  2|          9|\n|  2|  3|          9|\n|  2|  4|          9|\n|  4|  5|          5|\n|  6|  6|         13|\n|  7|  7|          7|\n+---+---+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "w1 = Window.orderBy(\"id\").rangeBetween(Window.currentRow, 1)  \n",
    "df1.withColumn(\"rolling_sum\", F.sum(\"num\").over(w1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9fb72fc-e34a-4005-b702-779a5242702a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----------+----+\n| id|num|rolling_sum|id*2|\n+---+---+-----------+----+\n|  1|  1|         10|   2|\n|  2|  2|          9|   4|\n|  2|  3|          9|   4|\n|  2|  4|          9|   4|\n|  4|  5|          5|   8|\n|  6|  6|         13|  12|\n|  7|  7|          7|  14|\n+---+---+-----------+----+\n\n"
     ]
    }
   ],
   "source": [
    "w2 = Window.orderBy(F.col(\"id\") * 2).rangeBetween(Window.currentRow, 1)  \n",
    "df1.withColumn(\"rolling_sum\", F.sum(\"num\").over(w1)).withColumn(\"id*2\", df1[\"id\"]*2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48356b8a-979d-4336-99dc-af94a5dd8949",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Shifting  a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ef3e5eb-2631-4820-9d88-8fcada790d26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "\u001B[0;32m<command-1442977062306290>\u001B[0m in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[0mw\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mWindow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0morderBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"time\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m----> 2\u001B[0;31m \u001B[0mdf1\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"lag_col\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlag\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"temperature\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mover\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n",
       "\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mwithColumn\u001B[0;34m(self, colName, col)\u001B[0m\n",
       "\u001B[1;32m   3325\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mColumn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   3326\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"col should be Column\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m-> 3327\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcolName\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparkSession\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m   3328\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   3329\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mwithColumnRenamed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexisting\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnew\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;34m\"DataFrame\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n",
       "\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n",
       "\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `temperature` cannot be resolved. Did you mean one of the following? [`num`, `id`];\n",
       "'Project [id#2817L, num#2818L, lag('temperature, -1, null) windowspecdefinition('time ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS lag_col#2904]\n",
       "+- Project [_1#2813L AS id#2817L, _2#2814L AS num#2818L]\n",
       "   +- LogicalRDD [_1#2813L, _2#2814L], false\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-1442977062306290>\u001B[0m in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mw\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mWindow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0morderBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"time\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mdf1\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"lag_col\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlag\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"temperature\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mover\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mwithColumn\u001B[0;34m(self, colName, col)\u001B[0m\n\u001B[1;32m   3325\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mColumn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3326\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"col should be Column\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3327\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcolName\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparkSession\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3328\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3329\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mwithColumnRenamed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexisting\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnew\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;34m\"DataFrame\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `temperature` cannot be resolved. Did you mean one of the following? [`num`, `id`];\n'Project [id#2817L, num#2818L, lag('temperature, -1, null) windowspecdefinition('time ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS lag_col#2904]\n+- Project [_1#2813L AS id#2817L, _2#2814L AS num#2818L]\n   +- LogicalRDD [_1#2813L, _2#2814L], false\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `temperature` cannot be resolved. Did you mean one of the following? [`num`, `id`];\n'Project [id#2817L, num#2818L, lag('temperature, -1, null) windowspecdefinition('time ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS lag_col#2904]\n+- Project [_1#2813L AS id#2817L, _2#2814L AS num#2818L]\n   +- LogicalRDD [_1#2813L, _2#2814L], false\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = Window.orderBy(\"time\")  \n",
    "df1.withColumn(\"lag_col\", F.lag(\"temperature\", 1).over(w)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c47efab3-0066-4bea-b615-23dfbc2fdd98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "\u001B[0;32m<command-1442977062306291>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdf1\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"lead_col\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlag\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"temperature\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mover\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n",
       "\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mwithColumn\u001B[0;34m(self, colName, col)\u001B[0m\n",
       "\u001B[1;32m   3325\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mColumn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   3326\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"col should be Column\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m-> 3327\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcolName\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparkSession\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m   3328\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   3329\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mwithColumnRenamed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexisting\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnew\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;34m\"DataFrame\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n",
       "\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n",
       "\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `temperature` cannot be resolved. Did you mean one of the following? [`num`, `id`];\n",
       "'Project [id#2817L, num#2818L, lag('temperature, 1, 0) windowspecdefinition('time ASC NULLS FIRST, specifiedwindowframe(RowFrame, 1, 1)) AS lead_col#2905]\n",
       "+- Project [_1#2813L AS id#2817L, _2#2814L AS num#2818L]\n",
       "   +- LogicalRDD [_1#2813L, _2#2814L], false\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-1442977062306291>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdf1\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"lead_col\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlag\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"temperature\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mover\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mwithColumn\u001B[0;34m(self, colName, col)\u001B[0m\n\u001B[1;32m   3325\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mColumn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3326\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"col should be Column\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3327\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcolName\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparkSession\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3328\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3329\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mwithColumnRenamed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexisting\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnew\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;34m\"DataFrame\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `temperature` cannot be resolved. Did you mean one of the following? [`num`, `id`];\n'Project [id#2817L, num#2818L, lag('temperature, 1, 0) windowspecdefinition('time ASC NULLS FIRST, specifiedwindowframe(RowFrame, 1, 1)) AS lead_col#2905]\n+- Project [_1#2813L AS id#2817L, _2#2814L AS num#2818L]\n   +- LogicalRDD [_1#2813L, _2#2814L], false\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `temperature` cannot be resolved. Did you mean one of the following? [`num`, `id`];\n'Project [id#2817L, num#2818L, lag('temperature, 1, 0) windowspecdefinition('time ASC NULLS FIRST, specifiedwindowframe(RowFrame, 1, 1)) AS lead_col#2905]\n+- Project [_1#2813L AS id#2817L, _2#2814L AS num#2818L]\n   +- LogicalRDD [_1#2813L, _2#2814L], false\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1.withColumn(\"lead_col\", F.lag(\"temperature\", -1, 0).over(w)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bd5c5bb-199c-4363-9d78-293be6a8dc97",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "\u001B[0;32m<command-1442977062306292>\u001B[0m in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[0mw\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mWindow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0morderBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"time\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m----> 2\u001B[0;31m \u001B[0mdf1\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"diff\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"temperature\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlag\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"temperature\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mover\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n",
       "\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mwithColumn\u001B[0;34m(self, colName, col)\u001B[0m\n",
       "\u001B[1;32m   3325\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mColumn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   3326\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"col should be Column\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m-> 3327\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcolName\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparkSession\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m   3328\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   3329\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mwithColumnRenamed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexisting\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnew\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;34m\"DataFrame\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n",
       "\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n",
       "\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `temperature` cannot be resolved. Did you mean one of the following? [`num`, `id`];\n",
       "'Project [id#2817L, num#2818L, ('temperature - lag('temperature, -1, null) windowspecdefinition('time ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1))) AS diff#2906]\n",
       "+- Project [_1#2813L AS id#2817L, _2#2814L AS num#2818L]\n",
       "   +- LogicalRDD [_1#2813L, _2#2814L], false\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-1442977062306292>\u001B[0m in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mw\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mWindow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0morderBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"time\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mdf1\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"diff\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"temperature\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlag\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"temperature\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mover\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mwithColumn\u001B[0;34m(self, colName, col)\u001B[0m\n\u001B[1;32m   3325\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mColumn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3326\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"col should be Column\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3327\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcolName\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparkSession\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3328\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3329\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mwithColumnRenamed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexisting\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnew\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;34m\"DataFrame\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `temperature` cannot be resolved. Did you mean one of the following? [`num`, `id`];\n'Project [id#2817L, num#2818L, ('temperature - lag('temperature, -1, null) windowspecdefinition('time ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1))) AS diff#2906]\n+- Project [_1#2813L AS id#2817L, _2#2814L AS num#2818L]\n   +- LogicalRDD [_1#2813L, _2#2814L], false\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `temperature` cannot be resolved. Did you mean one of the following? [`num`, `id`];\n'Project [id#2817L, num#2818L, ('temperature - lag('temperature, -1, null) windowspecdefinition('time ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1))) AS diff#2906]\n+- Project [_1#2813L AS id#2817L, _2#2814L AS num#2818L]\n   +- LogicalRDD [_1#2813L, _2#2814L], false\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = Window.orderBy(\"time\")\n",
    "df1.withColumn(\"diff\", F.col(\"temperature\") - F.lag(\"temperature\", 1).over(w)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13029e35-387e-48c5-a4ff-654259a7429f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Filling a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef1d31dd-f53d-478b-bcb5-6d6bf9ede5e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+\n|               time|temperature|\n+-------------------+-----------+\n|2020-01-01 07:30:00|       null|\n|2020-01-02 07:30:00|       25.5|\n|2020-01-03 07:30:00|       null|\n|2020-01-04 07:30:00|       21.2|\n|2020-01-05 07:30:00|       18.0|\n|2020-01-06 07:30:00|       null|\n+-------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "  (\"2020-01-01 07:30:00\", None), \n",
    "  (\"2020-01-02 07:30:00\", 25.5),  \n",
    "  (\"2020-01-03 07:30:00\", None),  \n",
    "  (\"2020-01-04 07:30:00\", 21.2),  \n",
    "  (\"2020-01-05 07:30:00\", 18.0), \n",
    "  (\"2020-01-06 07:30:00\", None)\n",
    "  ]\n",
    "\n",
    "df1 = spark.createDataFrame(data).toDF(\"time\", \"temperature\")\n",
    "df1 = df1.withColumn(\"time\", F.col(\"time\").cast('timestamp'))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7111116b-7695-4433-b3a3-79950d09ffd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+\n|               time|temperature|\n+-------------------+-----------+\n|2020-01-01 07:30:00|       null|\n|2020-01-02 07:30:00|       25.5|\n|2020-01-03 07:30:00|       25.5|\n|2020-01-04 07:30:00|       21.2|\n|2020-01-05 07:30:00|       18.0|\n|2020-01-06 07:30:00|       18.0|\n+-------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# ffill\n",
    "w = Window.orderBy('time')\n",
    "df2 = df1.select([F.last(c, ignorenulls=True).over(w).alias(c) for c in df1.columns])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee34ae16-b5fb-44a3-9992-e57b1268c1cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+\n|               time|temperature|\n+-------------------+-----------+\n|2020-01-01 07:30:00|       25.5|\n|2020-01-02 07:30:00|       25.5|\n|2020-01-03 07:30:00|       21.2|\n|2020-01-04 07:30:00|       21.2|\n|2020-01-05 07:30:00|       18.0|\n|2020-01-06 07:30:00|       null|\n+-------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# bfill\n",
    "w = Window.orderBy('time').rowsBetween(Window.currentRow, Window.unboundedFollowing)\n",
    "df2 = df1.select([F.first(c,ignorenulls=True).over(w).alias(c) for c in df1.columns])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04850736-0cdb-407e-bf4d-26f33525d7cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+\n|               time|temperature|\n+-------------------+-----------+\n|2020-01-01 07:30:00|       25.5|\n|2020-01-02 07:30:00|       25.5|\n|2020-01-03 07:30:00|       25.5|\n|2020-01-04 07:30:00|       21.2|\n|2020-01-05 07:30:00|       18.0|\n|2020-01-06 07:30:00|       18.0|\n+-------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# ffill.cfill\n",
    "w1 = Window.orderBy('time')\n",
    "w2 = w1.rowsBetween(Window.currentRow, Window.unboundedFollowing)\n",
    "df2 = df1.select([F.coalesce(F.last(c, ignorenulls=True).over(w1), F.first(c, ignorenulls=True).over(w2)).alias(c) for c in df1.columns])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d1bb4e9-e10b-40e7-b635-c50fc3281c15",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f263349-f177-4e32-a6d1-215e4d3ef1b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+\n| name|          role|salary|\n+-----+--------------+------+\n| John|Data scientist|  4500|\n|James| Data engineer|  3200|\n|Laura|Data scientist|  4100|\n|  Ali| Data engineer|  3200|\n|Steve|     Developer|  3600|\n+-----+--------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "data =[ \n",
    "  (\"John\", \"Data scientist\", 4500),\n",
    "  (\"James\", \"Data engineer\", 3200),\n",
    "  (\"Laura\", \"Data scientist\", 4100),\n",
    "  (\"Ali\", \"Data engineer\", 3200),\n",
    "  (\"Steve\", \"Developer\", 3600)\n",
    "]\n",
    "df = spark.createDataFrame(data).toDF(\"name\",\"role\", \"salary\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95897f2e-31e7-4503-8dcf-2c25d775076f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n| name|age|\n+-----+---+\n| John| 45|\n|James| 25|\n|Laura| 30|\n| Will| 28|\n+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "  (\"John\", 45),\n",
    "  (\"James\", 25),\n",
    "  (\"Laura\", 30),\n",
    "  (\"Will\", 28)\n",
    "]\n",
    "age_df = spark.createDataFrame(data).toDF(\"name\", \"age\")\n",
    "age_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36ca1bbc-819c-4bb2-95ea-b05e2b9977f4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Inner join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6279be24-8bd1-4887-8df4-0701477e5237",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+-----+---+\n| name|          role|salary| name|age|\n+-----+--------------+------+-----+---+\n| John|Data scientist|  4500| John| 45|\n|James| Data engineer|  3200|James| 25|\n|Laura|Data scientist|  4100|Laura| 30|\n+-----+--------------+------+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "joined_df = df.join(age_df, df[\"name\"] == age_df[\"name\"], \"inner\")\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cfd12ac-df9b-4f9b-84b7-2d5be8473731",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+---+\n| name|          role|salary|age|\n+-----+--------------+------+---+\n| John|Data scientist|  4500| 45|\n|James| Data engineer|  3200| 25|\n|Laura|Data scientist|  4100| 30|\n+-----+--------------+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "joined_df = df.join(age_df, [\"name\"], \"inner\")\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c57b2d08-8040-4b13-a288-7a4d308c98e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+---+\n| name|          role|salary|age|\n+-----+--------------+------+---+\n| John|Data scientist|  4500| 45|\n|James| Data engineer|  3200| 25|\n|Laura|Data scientist|  4100| 30|\n+-----+--------------+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "df.join(age_df, \"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f1c82d1-1bdd-42ae-b667-22e2db11e616",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+-----+---+\n| name|          role|salary| name|age|\n+-----+--------------+------+-----+---+\n| John|Data scientist|  4500| John| 45|\n|James| Data engineer|  3200|James| 25|\n|Laura|Data scientist|  4100|Laura| 30|\n+-----+--------------+------+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "joined_df = df.alias(\"df1\").join(age_df.alias(\"df2\"), col(\"df1.name\") == col(\"df2.name\"), \"inner\")\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87ee8213-f185-4863-9939-ec8cc69e175f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+-----+---+\n| name|          role|salary| name|age|\n+-----+--------------+------+-----+---+\n| John|Data scientist|  4500| John| 45|\n|James| Data engineer|  3200|James| 25|\n|Laura|Data scientist|  4100|Laura| 30|\n+-----+--------------+------+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "joined_df = df.alias(\"df1\").join(age_df.alias(\"df2\"), F.expr(\"df1.name == df2.name\"), \"inner\")\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96e170e6-cf61-456d-9eac-80dd4e3162cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+-----+--------------+------+\n| name|          role|salary| name|          role|salary|\n+-----+--------------+------+-----+--------------+------+\n|James| Data engineer|  3200| John|Data scientist|  4500|\n|James| Data engineer|  3200|Laura|Data scientist|  4100|\n|James| Data engineer|  3200|Steve|     Developer|  3600|\n|Laura|Data scientist|  4100| John|Data scientist|  4500|\n|  Ali| Data engineer|  3200| John|Data scientist|  4500|\n|  Ali| Data engineer|  3200|Laura|Data scientist|  4100|\n|  Ali| Data engineer|  3200|Steve|     Developer|  3600|\n|Steve|     Developer|  3600| John|Data scientist|  4500|\n|Steve|     Developer|  3600|Laura|Data scientist|  4100|\n+-----+--------------+------+-----+--------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "joined_df = df.alias(\"df1\").join(df.alias(\"df2\"), F.col(\"df1.salary\") < F.col(\"df2.salary\"), \"inner\")\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4962bf4-483f-4639-986e-51f27b137fd8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+-------------+---+\n| name|          role|salary|employee_name|age|\n+-----+--------------+------+-------------+---+\n| John|Data scientist|  4500|         John| 45|\n|James| Data engineer|  3200|        James| 25|\n|Laura|Data scientist|  4100|        Laura| 30|\n+-----+--------------+------+-------------+---+\n\n"
     ]
    }
   ],
   "source": [
    "age_df1 = age_df.withColumnRenamed(\"name\", \"employee_name\") \n",
    "joined_df = df.join(age_df1, F.expr(\"name == employee_name\"), \"inner\")\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c1a1fa3-1f58-43b7-8a93-87b7e47ed476",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+-----+---+\n| name|          role|salary| name|age|\n+-----+--------------+------+-----+---+\n| John|Data scientist|  4500| John| 45|\n|James| Data engineer|  3200|James| 25|\n|Laura|Data scientist|  4100|Laura| 30|\n+-----+--------------+------+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "joined_df = df.join(age_df).where(df[\"name\"] == age_df[\"name\"])\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0471014-170b-406d-b143-32579b514fce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Left and right joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e62642d3-73ef-4066-846d-b6401f2ec02d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+-----+----+\n| name|          role|salary| name| age|\n+-----+--------------+------+-----+----+\n| John|Data scientist|  4500| John|  45|\n|James| Data engineer|  3200|James|  25|\n|Laura|Data scientist|  4100|Laura|  30|\n|  Ali| Data engineer|  3200| null|null|\n|Steve|     Developer|  3600| null|null|\n+-----+--------------+------+-----+----+\n\n"
     ]
    }
   ],
   "source": [
    "joined_df = df.join(age_df, df[\"name\"] == age_df[\"name\"], \"left\")\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2d9389d-2f61-4674-abd5-4553ccc2380c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+----+\n| name|          role|salary| age|\n+-----+--------------+------+----+\n| John|Data scientist|  4500|  45|\n|James| Data engineer|  3200|  25|\n|Laura|Data scientist|  4100|  30|\n|  Ali| Data engineer|  3200|null|\n|Steve|     Developer|  3600|null|\n+-----+--------------+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "joined_df = df.join(age_df, \"name\", \"left\")\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "919dd664-e6e4-484d-8b87-ca3aafbd10c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+---+\n| name|          role|salary|age|\n+-----+--------------+------+---+\n| John|Data scientist|  4500| 45|\n|James| Data engineer|  3200| 25|\n|Laura|Data scientist|  4100| 30|\n| Will|          null|  null| 28|\n+-----+--------------+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "joined_df = df.join(age_df, [\"name\"], \"right\")\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a83e9619-9f07-4fa9-b5c6-791ffbc34cf8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Full join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d8b8cd2-0e4a-4456-bea6-a2de36d587f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+----+\n| name|          role|salary| age|\n+-----+--------------+------+----+\n|  Ali| Data engineer|  3200|null|\n|James| Data engineer|  3200|  25|\n| John|Data scientist|  4500|  45|\n|Laura|Data scientist|  4100|  30|\n|Steve|     Developer|  3600|null|\n| Will|          null|  null|  28|\n+-----+--------------+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "df.join(age_df, \"name\", \"full\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72e75822-d824-4481-84f8-56016ab4b367",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Left semi join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80e547b8-ddb6-4a04-ba80-dc4a56deef9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+\n| name|          role|salary|\n+-----+--------------+------+\n| John|Data scientist|  4500|\n|James| Data engineer|  3200|\n|Laura|Data scientist|  4100|\n+-----+--------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "joined_df = df.join(age_df, df[\"name\"] == age_df[\"name\"], \"leftsemi\")\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d40afdb-aeb9-4f00-a52f-194c833a9578",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Left anti join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cf42602-fd24-4413-8b84-de1047c2b881",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+------+\n| name|         role|salary|\n+-----+-------------+------+\n|  Ali|Data engineer|  3200|\n|Steve|    Developer|  3600|\n+-----+-------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "joined_df = df.join(age_df, df[\"name\"] == age_df[\"name\"], \"leftanti\")\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2acebdd5-acf2-481d-8fa6-2ac33cc7290f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Cross join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0901b842-cf16-413a-94d9-ad012115e67d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+-----+---+\n| name|          role|salary| name|age|\n+-----+--------------+------+-----+---+\n| John|Data scientist|  4500| John| 45|\n|James| Data engineer|  3200|James| 25|\n|Laura|Data scientist|  4100|Laura| 30|\n+-----+--------------+------+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "joined_df = df.join(age_df, df[\"name\"] == age_df[\"name\"], \"cross\")\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "541e0778-ff42-4ef7-9ee2-845375e23931",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+-----+---+\n| name|          role|salary| name|age|\n+-----+--------------+------+-----+---+\n| John|Data scientist|  4500| John| 45|\n| John|Data scientist|  4500|James| 25|\n| John|Data scientist|  4500|Laura| 30|\n| John|Data scientist|  4500| Will| 28|\n|James| Data engineer|  3200| John| 45|\n|James| Data engineer|  3200|James| 25|\n|James| Data engineer|  3200|Laura| 30|\n|James| Data engineer|  3200| Will| 28|\n|Laura|Data scientist|  4100| John| 45|\n|Laura|Data scientist|  4100|James| 25|\n|Laura|Data scientist|  4100|Laura| 30|\n|Laura|Data scientist|  4100| Will| 28|\n|  Ali| Data engineer|  3200| John| 45|\n|  Ali| Data engineer|  3200|James| 25|\n|  Ali| Data engineer|  3200|Laura| 30|\n|  Ali| Data engineer|  3200| Will| 28|\n|Steve|     Developer|  3600| John| 45|\n|Steve|     Developer|  3600|James| 25|\n|Steve|     Developer|  3600|Laura| 30|\n|Steve|     Developer|  3600| Will| 28|\n+-----+--------------+------+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "joined_df = df.crossJoin(age_df)\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f21f1f3-6d55-4029-9422-502e5e929789",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Concatenating DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7a0464d-64d8-468f-af26-ea5a4984ac19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n|col0|col1|col2|\n+----+----+----+\n|   1|   2|   a|\n|   3|   4|   b|\n+----+----+----+\n\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(1, 2, \"a\")]).toDF(\"col0\", \"col1\", \"col2\")\n",
    "df2 = spark.createDataFrame([(3, 4, \"b\")]).toDF(\"col0\", \"col1\", \"col2\")\n",
    "df1.union(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54774f1d-e908-415b-a885-cd55e74f3cba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n|col0|col1|col2|\n+----+----+----+\n|   1|   2|   a|\n|   b|   3|   4|\n+----+----+----+\n\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(1, 2, \"a\")]).toDF(\"col0\", \"col1\", \"col2\")\n",
    "df2 = spark.createDataFrame([(\"b\", 3, 4)]).toDF(\"col2\", \"col0\", \"col1\")\n",
    "df1.union(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e2c61d6-8334-4d06-bdfa-217e9e909cd0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n|col0|col1|col2|\n+----+----+----+\n|   1|   2|   a|\n|   3|   4|   b|\n+----+----+----+\n\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(1, 2, \"a\")]).toDF(\"col0\", \"col1\", \"col2\")\n",
    "df2 = spark.createDataFrame([(\"b\", 3, 4)]).toDF(\"col2\", \"col0\", \"col1\")\n",
    "df1.unionByName(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9eaf473-04f7-464b-8dd5-6b34a3b96887",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n|col0|col1|col3|col4|\n+----+----+----+----+\n|   1|   2|   a|   b|\n|   3|   4|   c|   d|\n+----+----+----+----+\n\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(1, 2), (3, 4)]).toDF(\"col0\", \"col1\")\n",
    "df2 = spark.createDataFrame([(\"a\", \"b\"), (\"c\", \"d\")]).toDF(\"col3\", \"col4\")\n",
    "\n",
    "df1.withColumn(\"row_id\", F.monotonically_increasing_id()) \\\n",
    "   .join(df2.withColumn(\"row_id\", F.monotonically_increasing_id()), \"row_id\") \\\n",
    "   .drop(\"row_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6051a99-1395-4864-882f-3b5e367be02a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ec80e81-7d98-4cc3-8dfb-4f5873a55440",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n|               time|value|\n+-------------------+-----+\n|2020-01-02 01:00:00|    1|\n|2020-01-02 01:20:00|    2|\n|2020-01-02 01:40:00|    4|\n|2020-01-02 02:00:00|    5|\n|2020-01-02 02:20:00|    6|\n|2020-01-02 02:40:00|    7|\n|2020-01-02 03:00:00|    8|\n|2020-01-02 03:20:00|    9|\n|2020-01-02 03:40:00|   10|\n|2020-01-02 04:00:00|   11|\n|2020-01-02 04:20:00|   12|\n|2020-01-02 04:40:00|   13|\n|2020-01-02 05:00:00|   14|\n+-------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "  (\"2020-01-02 01:00:00\", 1), \n",
    "  (\"2020-01-02 01:20:00\", 2),  \n",
    "  (\"2020-01-02 01:40:00\", 4),  \n",
    "  (\"2020-01-02 02:00:00\", 5),  \n",
    "  (\"2020-01-02 02:20:00\", 6), \n",
    "  (\"2020-01-02 02:40:00\", 7),\n",
    "  (\"2020-01-02 03:00:00\", 8), \n",
    "  (\"2020-01-02 03:20:00\", 9),  \n",
    "  (\"2020-01-02 03:40:00\", 10),  \n",
    "  (\"2020-01-02 04:00:00\", 11),  \n",
    "  (\"2020-01-02 04:20:00\", 12), \n",
    "  (\"2020-01-02 04:40:00\", 13),\n",
    "  (\"2020-01-02 05:00:00\", 14),\n",
    "  ]\n",
    "\n",
    "df1 = spark.createDataFrame(data).toDF(\"time\", \"value\")\n",
    "df1 = df1.withColumn(\"time\", F.col(\"time\").cast(\"timestamp\"))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2eb935b-e9f3-4f86-a8d8-171489cbaa37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+\n|               time|sum(value)|\n+-------------------+----------+\n|2020-01-02 01:00:00|         7|\n|2020-01-02 02:00:00|        18|\n|2020-01-02 03:00:00|        27|\n|2020-01-02 04:00:00|        36|\n|2020-01-02 05:00:00|        14|\n+-------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# df1.resample('1H', closed='left', label = 'left').sum()\n",
    "df1.groupBy(F.date_trunc('hour', df1['time']).alias('time')) \\\n",
    "   .sum().orderBy('time').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12c9f71a-c1ab-47db-b9f0-f3b711c081cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+\n|               time|sum(value)|\n+-------------------+----------+\n|2020-01-02 02:00:00|         7|\n|2020-01-02 03:00:00|        18|\n|2020-01-02 04:00:00|        27|\n|2020-01-02 05:00:00|        36|\n|2020-01-02 06:00:00|        14|\n+-------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# df1.resample('1H', closed='left', label = 'right').sum()\n",
    "df1.groupBy((F.date_trunc('hour', df1['time'])+F.expr('INTERVAL 60 MINUTES')) \\\n",
    "   .alias('time')).sum().orderBy('time').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2be3134a-7f52-4f7a-915e-a276ee9b8af3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+\n|               time|sum(value)|\n+-------------------+----------+\n|2020-01-02 01:00:00|         1|\n|2020-01-02 02:00:00|        11|\n|2020-01-02 03:00:00|        21|\n|2020-01-02 04:00:00|        30|\n|2020-01-02 05:00:00|        39|\n+-------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# df1.resample('1H', closed='right', label = 'right').sum()\n",
    "diff = col('time').cast('long') - F.date_trunc('hour', col('time')).cast('long')\n",
    "ind = F.when(diff == 0, col('time')) \\\n",
    "       .otherwise(F.date_trunc('hour', col('time')+F.expr('INTERVAL 60 MINUTES')))\n",
    "df1.groupBy(ind.alias('time')) \\\n",
    "   .sum().orderBy('time').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37aa138e-219f-490f-90af-ef999701c768",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+\n|               time|sum(value)|\n+-------------------+----------+\n|2020-01-02 00:00:00|         1|\n|2020-01-02 01:00:00|        11|\n|2020-01-02 02:00:00|        21|\n|2020-01-02 03:00:00|        30|\n|2020-01-02 04:00:00|        39|\n+-------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# df1.resample('1H', closed='right', label = 'left').sum()\n",
    "diff = col('time').cast('long') - F.date_trunc('hour', col('time')).cast('long')\n",
    "ind = F.when(diff == 0, col('time')) \\\n",
    "       .otherwise(F.date_trunc('hour', col('time')+F.expr('INTERVAL 60 MINUTES')))\n",
    "df1.groupBy((ind-F.expr('INTERVAL 60 MINUTES')).alias('time')) \\\n",
    "           .sum().orderBy('time').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3411059d-9c61-490c-b65a-afcd6535d7ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n|               time|value|\n+-------------------+-----+\n|2020-01-01 01:00:00|    1|\n|2020-01-01 05:00:00|    2|\n|2020-01-01 10:00:00|    3|\n+-------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "  (\"2020-01-01 01:00:00\", 1), \n",
    "  (\"2020-01-01 05:00:00\", 2),  \n",
    "  (\"2020-01-01 10:00:00\", 3),  \n",
    "  ]\n",
    "\n",
    "df1 = spark.createDataFrame(data).toDF(\"time\", \"value\")\n",
    "df1 = df1.withColumn(\"time\", F.col(\"time\").cast(\"timestamp\"))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c2c8ded-a5ab-4891-b09b-76fdd1b0ec65",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n|               time|value|\n+-------------------+-----+\n|2020-01-01 01:00:00|    1|\n|2020-01-01 02:00:00| null|\n|2020-01-01 03:00:00| null|\n|2020-01-01 04:00:00| null|\n|2020-01-01 05:00:00|    2|\n|2020-01-01 06:00:00| null|\n|2020-01-01 07:00:00| null|\n|2020-01-01 08:00:00| null|\n|2020-01-01 09:00:00| null|\n|2020-01-01 10:00:00|    3|\n+-------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "hour = 60 * 60 \n",
    "epoch = (col(\"time\").cast(\"long\") / hour).cast(\"long\") * hour\n",
    "with_epoch = df1.withColumn(\"epoch\", epoch)\n",
    "min_epoch, max_epoch = with_epoch.select(F.min(\"epoch\"), F.max(\"epoch\")).first()\n",
    "\n",
    "time_range = spark.range(min_epoch, max_epoch + 1, hour).toDF(\"epoch\")\n",
    "\n",
    "time_range.join(with_epoch.select(['epoch', 'value']), \"epoch\", \"left\") \\\n",
    "          .orderBy(\"epoch\").withColumn(\"time\", col(\"epoch\") \\\n",
    "          .cast(\"timestamp\")).drop('epoch') \\\n",
    "          .select(['time', 'value']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d341fa22-ca30-4ff5-b0e8-211028119ec8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark_dataframes",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
